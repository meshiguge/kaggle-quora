{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('vacabulary_subs.pkl', 'r') as output:\n",
    "#     vacabulary_subs = pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train['q1_spelling_check'] = df_train.question1.apply(lambda x: ' '.join([vacabulary_subs.get(w, w) for w in text_to_wordlist(x).split()]))\n",
    "# df_train['q2_spelling_check'] = df_train.question2.apply(lambda x: ' '.join([vacabulary_subs.get(w, w) for w in text_to_wordlist(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_test['q1_spelling_check'] = df_test.question1.apply(lambda x: ' '.join([vacabulary_subs.get(w, w) for w in text_to_wordlist(x).split()]))\n",
    "# df_test['q2_spelling_check'] = df_test.question2.apply(lambda x: ' '.join([vacabulary_subs.get(w, w) for w in text_to_wordlist(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation='[\"\\'?,\\.]' # I will replace all these punctuation with ''\n",
    "abbr_dict={\n",
    "    \"what's\":\"what is\",\n",
    "    \"what're\":\"what are\",\n",
    "    \"who's\":\"who is\",\n",
    "    \"who're\":\"who are\",\n",
    "    \"where's\":\"where is\",\n",
    "    \"where're\":\"where are\",\n",
    "    \"when's\":\"when is\",\n",
    "    \"when're\":\"when are\",\n",
    "    \"how's\":\"how is\",\n",
    "    \"how're\":\"how are\",\n",
    "\n",
    "    \"i'm\":\"i am\",\n",
    "    \"we're\":\"we are\",\n",
    "    \"you're\":\"you are\",\n",
    "    \"they're\":\"they are\",\n",
    "    \"it's\":\"it is\",\n",
    "    \"he's\":\"he is\",\n",
    "    \"she's\":\"she is\",\n",
    "    \"that's\":\"that is\",\n",
    "    \"there's\":\"there is\",\n",
    "    \"there're\":\"there are\",\n",
    "\n",
    "    \"i've\":\"i have\",\n",
    "    \"we've\":\"we have\",\n",
    "    \"you've\":\"you have\",\n",
    "    \"they've\":\"they have\",\n",
    "    \"who've\":\"who have\",\n",
    "    \"would've\":\"would have\",\n",
    "    \"not've\":\"not have\",\n",
    "\n",
    "    \"i'd\":\"i had\",\n",
    "    \"we'd\":\"we had\",\n",
    "    \"you'd\":\"you had\",\n",
    "    \"they'd\":\"they had\",\n",
    "    \"who'd\":\"who had\",\n",
    "    \n",
    "    \"i'll\":\"i will\",\n",
    "    \"we'll\":\"we will\",\n",
    "    \"you'll\":\"you will\",\n",
    "    \"he'll\":\"he will\",\n",
    "    \"she'll\":\"she will\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"they'll\":\"they will\",\n",
    "\n",
    "    \"isn't\":\"is not\",\n",
    "    \"wasn't\":\"was not\",\n",
    "    \"aren't\":\"are not\",\n",
    "    \"weren't\":\"were not\",\n",
    "    \"can't\":\"can not\",\n",
    "    \"couldn't\":\"could not\",\n",
    "    \"don't\":\"do not\",\n",
    "    \"didn't\":\"did not\",\n",
    "    \"shouldn't\":\"should not\",\n",
    "    \"wouldn't\":\"would not\",\n",
    "    \"doesn't\":\"does not\",\n",
    "    \"haven't\":\"have not\",\n",
    "    \"hasn't\":\"has not\",\n",
    "    \"hadn't\":\"had not\",\n",
    "    \"won't\":\"will not\",\n",
    "    punctuation:'',\n",
    "    '\\s+':' ', # replace multi space with one single space\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(file_name):\n",
    "    data=pd.read_csv(file_name)\n",
    "    data.question1=data.question1.str.lower() # conver to lower case\n",
    "    data.question2=data.question2.str.lower()\n",
    "    data.question1=data.question1.astype(str)\n",
    "    data.question2=data.question2.astype(str)\n",
    "    data.replace(abbr_dict,regex=True,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df=process_data('train_org.csv')\n",
    "test_df=process_data('test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "import codecs\n",
    "import csv\n",
    "import re\n",
    "#from gensim.models import KeyedVectors\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    if type(text)==float:\n",
    "        text = str(text)\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    text = text.lower()\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.strip()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text.split() if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    #text = \" \".join(text)\n",
    "    #text = text.strip()\n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_clean =  train_df.question1.apply(lambda x: text_to_wordlist(x, remove_stopwords=False, stem_words=False))\n",
    "q1_no_stops =  train_df.question1.apply(lambda x: text_to_wordlist(x, remove_stopwords=True, stem_words=False))\n",
    "q1_stems =  train_df.question1.apply(lambda x: text_to_wordlist(x, remove_stopwords=False, stem_words=True))\n",
    "q1_stems_no_stops =  train_df.question1.apply(lambda x: text_to_wordlist(x, remove_stopwords=True, stem_words=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['q1_clean'] = q1_clean\n",
    "train_df['q1_no_stops'] = q1_no_stops\n",
    "train_df['q1_stems'] = q1_stems\n",
    "train_df['q1_stems_no_stops'] = q1_stems_no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['q2_clean'] = train_df.question2.apply(lambda x: text_to_wordlist(x, remove_stopwords=False, stem_words=False))\n",
    "train_df['q2_no_stops'] = train_df.question2.apply(lambda x: text_to_wordlist(x, remove_stopwords=True, stem_words=False))\n",
    "train_df['q2_stems'] = train_df.question2.apply(lambda x: text_to_wordlist(x, remove_stopwords=False, stem_words=True))\n",
    "train_df['q2_stems_no_stops'] = train_df.question2.apply(lambda x: text_to_wordlist(x, remove_stopwords=True, stem_words=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_clean_data(df):\n",
    "    df['q1_clean'] = df.question1.apply(lambda x: text_to_wordlist(x, remove_stopwords=False, stem_words=False))\n",
    "    df['q1_no_stops'] = df.question1.apply(lambda x: text_to_wordlist(x, remove_stopwords=True, stem_words=False))\n",
    "    df['q1_stems'] = df.question1.apply(lambda x: text_to_wordlist(x, remove_stopwords=False, stem_words=True))\n",
    "    df['q1_stems_no_stops'] = df.question1.apply(lambda x: text_to_wordlist(x, remove_stopwords=True, stem_words=True))\n",
    "    df['q2_clean'] = df.question2.apply(lambda x: text_to_wordlist(x, remove_stopwords=False, stem_words=False))\n",
    "    df['q2_no_stops'] = df.question2.apply(lambda x: text_to_wordlist(x, remove_stopwords=True, stem_words=False))\n",
    "    df['q2_stems'] = df.question2.apply(lambda x: text_to_wordlist(x, remove_stopwords=False, stem_words=True))\n",
    "    df['q2_stems_no_stops'] = df.question2.apply(lambda x: text_to_wordlist(x, remove_stopwords=True, stem_words=True))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_clean  = gen_clean_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df= pd.read_csv('./processed_data/train_clean.csv')\n",
    "test_clean = pd.read_csv('./processed_data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist.append('till')  # add 'till' to stoplist\n",
    "# 'can' also might mean 'a container' like in 'trash can' \n",
    "# so we create a separate stop list without 'can' to be used for query and product title\n",
    "stoplist_wo_can=stoplist[:]\n",
    "#stoplist_wo_can.remove('can')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def str_stemmer_wo_parser(s, stoplist=stoplist):\n",
    "    s=\" \".join([word for word in s.split() if word not in stoplist])\n",
    "    return \" \".join([stemmer.stem(re.sub('\\.(?=$)', '', word)) for word in s.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['q1_stemmed']=train_df['question1'].apply(lambda x:str_stemmer_wo_parser(str(x).decode('utf-8'),stoplist=stoplist_wo_can))\n",
    "train_df['q2_stemmed']=train_df['question2'].apply(lambda x:str_stemmer_wo_parser(str(x).decode('utf-8'),stoplist=stoplist_wo_can))\n",
    "test_clean['q1_stemmed']=test_clean['question1'].apply(lambda x:str_stemmer_wo_parser(str(x).decode('utf-8'),stoplist=stoplist_wo_can))\n",
    "test_clean['q2_stemmed']=test_clean['question2'].apply(lambda x:str_stemmer_wo_parser(str(x).decode('utf-8'),stoplist=stoplist_wo_can))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['q2_stems_difference'] = train_df.apply(lambda x: ' '.join([s for s  in str(x['q2_stems_no_stops']).split() if s not in set(str(x['q2_stems_no_stops']).split()).intersection(set(str(x['q1_stems_no_stops']).split()))]), axis=1)\n",
    "train_df['q1_stems_difference'] = train_df.apply(lambda x: ' '.join([s for s  in str(x['q1_stems_no_stops']).split() if s not in set(str(x['q2_stems_no_stops']).split()).intersection(set(str(x['q1_stems_no_stops']).split()))]), axis=1)\n",
    "train_df['q1_no_stops_difference'] = train_df.apply(lambda x: ' '.join([s for s  in str(x['q1_no_stops']).split() if s not in set(str(x['q1_no_stops']).split()).intersection(set(str(x['q2_no_stops']).split()))]), axis=1)\n",
    "train_df['q2_no_stops_difference'] = train_df.apply(lambda x: ' '.join([s for s  in str(x['q2_no_stops']).split() if s not in set(str(x['q1_no_stops']).split()).intersection(set(str(x['q2_no_stops']).split()))]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_clean['q2_stems_difference'] = test_clean.apply(lambda x: ' '.join([s for s  in str(x['q2_stems_no_stops']).split() if s not in set(str(x['q2_stems_no_stops']).split()).intersection(set(str(x['q1_stems_no_stops']).split()))]), axis=1)\n",
    "test_clean['q1_stems_difference'] = test_clean.apply(lambda x: ' '.join([s for s  in str(x['q1_stems_no_stops']).split() if s not in set(str(x['q2_stems_no_stops']).split()).intersection(set(str(x['q1_stems_no_stops']).split()))]), axis=1)\n",
    "test_clean['q2_no_stops_difference'] = test_clean.apply(lambda x: ' '.join([s for s  in str(x['q2_no_stops']).split() if s not in set(str(x['q1_no_stops']).split()).intersection(set(str(x['q2_no_stops']).split()))]), axis=1)\n",
    "test_clean['q1_no_stops_difference'] = test_clean.apply(lambda x: ' '.join([s for s  in str(x['q1_no_stops']).split() if s not in set(str(x['q1_no_stops']).split()).intersection(set(str(x['q2_no_stops']).split()))]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['q1_clean_difference'] = train_df.apply(lambda x: ' '.join([s for s  in str(x['q1_clean']).split() if s not in set(str(x['q2_clean']).split()).intersection(set(str(x['q1_clean']).split()))]), axis=1)\n",
    "train_df['q2_clean_difference'] = train_df.apply(lambda x: ' '.join([s for s  in str(x['q2_clean']).split() if s not in set(str(x['q1_clean']).split()).intersection(set(str(x['q2_clean']).split()))]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def get_nouns(df_):\n",
    "    data = pd.DataFrame()\n",
    "    data['question1_nouns'] = df_.question1.map(lambda x: ' '.join([w for w, t in nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8').strip().lower() if type(x)!=float else str(x))) if t[:1] in ['N']]))\n",
    "    data['question2_nouns'] = df_.question2.map(lambda x: ' '.join([w for w, t in nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8').strip().lower() if type(x)!=float else str(x))) if t[:1] in ['N']]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./processed_data/train_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./processed_data/test_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_n_train = get_nouns(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_n_test = get_nouns(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_n_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def joindata(df_):\n",
    "#     data = pd.DataFrame()\n",
    "#     data['question1_nouns'] = df_.question1_nouns.apply(lambda x: ' '.join(x))\n",
    "#     data['question2_nouns'] = df_.question2_nouns.apply(lambda x: ' '.join(x))\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_train_noun = joindata(data_n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_test_noun = joindata(data_n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def get_nouns(df_):\n",
    "    data = pd.DataFrame()\n",
    "    data['question1_nouns'] = df_.question1.map(lambda x: ' '.join([w for w, t in nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8').strip().lower() if type(x)!=float else str(x))) if t[:1] in ['N']]))\n",
    "    data['question2_nouns'] = df_.question2.map(lambda x: ' '.join([w for w, t in nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8').strip().lower() if type(x)!=float else str(x))) if t[:1] in ['N']]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['q1_no_stops_difference'] = train_df.apply(lambda x: ' '.join([s for s  in str(x['q1_no_stops']).split() if s not in set(str(x['q1_no_stops']).split()).intersection(set(str(x['q2_no_stops']).split()))]), axis=1)\n",
    "train_df['q2_no_stops_difference'] = train_df.apply(lambda x: ' '.join([s for s  in str(x['q2_no_stops']).split() if s not in set(str(x['q1_no_stops']).split()).intersection(set(str(x['q2_no_stops']).split()))]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def replace_nan(s):\n",
    "    if pd.isnull(s)==True or s=='nan':\n",
    "        s=\"<PAD/>\"\n",
    "    return s\n",
    "for c in data_train_noun.columns:\n",
    "    data_train_noun[c] = data_train_noun[c].apply(lambda x:replace_nan(x))\n",
    "for c in data_test_noun.columns:\n",
    "    data_test_noun[c] = data_test_noun[c].apply(lambda x:replace_nan(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in data_train_noun.columns:\n",
    "    train[c] = data_train_noun[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in data_test_noun.columns:\n",
    "    test[c] = data_test_noun[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.to_csv('./processed_data/test_clean.csv',index=False,encoding='utf-8')\n",
    "train.to_csv('./processed_data/train_clean.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_clean.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_keep = ['JJ','JJS','JJR','NN','NNP','NNPS','NNS','VB','VBD','VBG','VBP','VBZ','FW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[w for w, t in nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8').strip().lower() if type(x)!=float else str(x))) if t in list_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def get_nouns_verb_adj(df_):\n",
    "    data = pd.DataFrame()\n",
    "    data['question1_nouns'] = df_.question1.apply(lambda x: ' '.join([w for w, t in nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8').strip().lower() if type(x)!=float else str(x))) if t in list_keep]))\n",
    "    data['question2_nouns'] = df_.question2.apply(lambda x: ' '.join([w for w, t in nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8').strip().lower() if type(x)!=float else str(x))) if t in list_keep]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_extract_train = get_nouns_verb_adj(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_extract_test = get_nouns_verb_adj(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data_extract_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in data_extract_train.columns:\n",
    "    train[c] = data_extract_train[c] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in data_extract_test.columns:\n",
    "    test[c] = data_extract_test[c] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def get_nouns_verb_adj_nostops(df_):\n",
    "    data = pd.DataFrame()\n",
    "    data['question1_nouns_verb_adj_nostops'] = df_.apply(lambda x: ' '.join([s for s  in str(x['question1_nouns']).split() if s not in stops]),axis=1)\n",
    "    data['question2_nouns_verb_adj_nostops'] = df_.apply(lambda x: ' '.join([s for s  in str(x['question2_nouns']).split() if s not in stops]),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_bo_stops_anv = get_nouns_verb_adj_nostops(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_bo_stops_anv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_bo_stops_anv = get_nouns_verb_adj_nostops(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_nan(s):\n",
    "    if pd.isnull(s)==True or s=='nan':\n",
    "        s=\"<PAD/>\"\n",
    "    return s\n",
    "for c in train_bo_stops_anv.columns:\n",
    "    train_bo_stops_anv[c] = train_bo_stops_anv[c].apply(lambda x:replace_nan(x))\n",
    "for c in test_bo_stops_anv.columns:\n",
    "    test_bo_stops_anv[c] = test_bo_stops_anv[c].apply(lambda x:replace_nan(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in train_bo_stops_anv.columns:\n",
    "    train[c] = train_bo_stops_anv[c] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in test_bo_stops_anv.columns:\n",
    "    test[c] = test_bo_stops_anv[c] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_csv('./processed_data/test_clean.csv',index=False,encoding='utf-8')\n",
    "train.to_csv('./processed_data/train_clean.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_extract_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def get_pos(df_):\n",
    "    data = pd.DataFrame()\n",
    "    data['question1_pos'] = df_.question1.map(lambda x: [t for w, t in nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8').strip().lower() if type(x)!=float else str(x)))])\n",
    "    data['question2_pos'] = df_.question2.map(lambda x: [t for w, t in nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8').strip().lower() if type(x)!=float else str(x)))])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos = get_pos(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def get_pos_words(df_):\n",
    "    data = pd.DataFrame()\n",
    "    data['question1_pos_words'] = df_.question1.map(lambda x: [w for w, t in nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8').strip().lower() if type(x)!=float else str(x)))])\n",
    "    data['question2_pos_words'] = df_.question2.map(lambda x: [w for w, t in nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8').strip().lower() if type(x)!=float else str(x)))])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_words = get_pos_words(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_pos_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_pos.to_csv('./processed_data/poswords_pos.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_words.to_csv('./processed_data/poswords.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('./processed_data/test_clean.csv',usecols=['question2_nouns_verb_adj_nostops','question2_nouns_verb_adj_nostops'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./processed_data/train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train[[u'question1_nouns_verb_adj_nostops',\n",
    "       u'question2_nouns_verb_adj_nostops']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
