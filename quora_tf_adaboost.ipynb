{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train = df_train.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "def build_vocab(sentences, max_vocab=30000):\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    vocabulary_inv = []\n",
    "    vocabulary_inv.append(\"<PAD/>\")\n",
    "    vocabulary_inv.append(\"<mino/>\")\n",
    "    vocabulary_inv.extend([x[0] for x in word_counts.most_common(max_vocab)])\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def nltk_tokenize(s):    \n",
    "    s = s.lower().strip()    \n",
    "    return nltk.word_tokenize(s)\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.,!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s.split(' ')\n",
    "# Split every line into pairs and normalize\n",
    "pairs = []\n",
    "labels = []\n",
    "for q1,q2,is_dup in zip(df_train.question1,df_train.question2,df_train.is_duplicate):\n",
    "#     if type(q1) == unicode:\n",
    "#         print(\"unicode\")\n",
    "#         break\n",
    "#     if type(q2) == unicode:\n",
    "#         print(\"unicode\")\n",
    "#         break\n",
    "#     if type(q1) == unicode:\n",
    "#         pair1 = unicodeToAscii(q1)\n",
    "#     else:\n",
    "#         pair1 = q1\n",
    "#     if type(q2) == unicode:\n",
    "#     pair2 = unicodeToAscii(q2)\n",
    "#     else:\n",
    "#         pair2 = q2\n",
    "    try:\n",
    "        pair1 =  nltk_tokenize(q1)\n",
    "    except:\n",
    "        pair1 =  normalizeString(q1)\n",
    "    if type(q2) ==float:\n",
    "        q2 = \"<mino/>\"\n",
    "    try:\n",
    "        pair2 =  nltk_tokenize(q2)\n",
    "    except:\n",
    "        pair2 =  normalizeString(q2)\n",
    "#     newpair1 =pair1.split(' ')\n",
    "#     newpair2 =pair2.split(' ')\n",
    "    pairs.append([pair1,pair2,is_dup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocabulary, vocabulary_inv = build_vocab(np.array(pairs)[:,0] + np.array(pairs)[:,1], max_vocab=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('vacabulary_quora_adab.pkl', 'wb')\n",
    "pickle.dump(vocabulary,output)\n",
    "output.close()\n",
    "output = open('vocabulary_inv_quora_adab.pkl', 'wb') \n",
    "pickle.dump(vocabulary,output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_data(input_data, vocabulary,training = True):\n",
    "    mino_idx = vocabulary[\"<mino/>\"]\n",
    "    data_index = []\n",
    "    for pair in input_data:\n",
    "        sentence1 = [vocabulary.get(word, mino_idx) for word in pair[0]]\n",
    "        sentence2 = [vocabulary.get(word, mino_idx) for word in pair[1]]\n",
    "        if training:\n",
    "            data_index.append([sentence1,sentence2,pair[2]])\n",
    "        else:\n",
    "            data_index.append([sentence1,sentence2])\n",
    "    return data_index\n",
    "data_index = build_data(pairs,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(batch,training = True):\n",
    "    length1 = [len(sen1) for sen1 in batch[:,0]]\n",
    "    max_length1 = max(length1)\n",
    "    length2 = [len(sen2) for sen2 in batch[:,1]]\n",
    "    max_length2 = max(length2)\n",
    "    s1_batch = []\n",
    "    s2_batch = []\n",
    "    for sen in batch:\n",
    "        if len(sen[0])==max_length1:\n",
    "            s1_batch.append(sen[0])\n",
    "        else:\n",
    "            s1_batch.append(sen[0]+(max_length1-len(sen[0]))*[0])\n",
    "        if len(sen[1])==max_length2:\n",
    "            s2_batch.append(sen[1])\n",
    "        else:\n",
    "            s2_batch.append(sen[1]+(max_length2-len(sen[1]))*[0])\n",
    "    if training:   \n",
    "        return np.array(s1_batch),np.array(s2_batch),batch[:,2],length1,length2\n",
    "    else:\n",
    "        return np.array(s1_batch),np.array(s2_batch),length1,length2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(input, output_dim, scope=None, stddev=0.1):\n",
    "    norm = tf.random_normal_initializer(stddev=stddev)\n",
    "    const = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope(scope or 'linear'):\n",
    "        w = tf.get_variable('w', [input.get_shape()[1], output_dim], initializer=norm)\n",
    "        b = tf.get_variable('b', [output_dim], initializer=const)\n",
    "        return tf.matmul(input, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "class LSTM(object):\n",
    "    \"\"\" \n",
    "    a simple implement of lstm\n",
    "    \"\"\"\n",
    "    def __init__(self,embedding_size = 300,word_vocab_size=6726):\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.input_s1 = tf.placeholder(tf.int64, [None, None], name='input_placeholder_x1')\n",
    "        self.input_s2 = tf.placeholder(tf.int64, [None, None], name='input_placeholder_x2')\n",
    "        self.input_y = tf.placeholder(tf.int64, [None], name='input_placeholder_label')\n",
    "        \n",
    "        self.y_onehot = tf.to_float(tf.one_hot(tf.reshape(self.input_y,[-1]), 2, on_value=1, off_value=0, axis=-1,\n",
    "                             dtype=tf.int32, name='y_oneHot'))\n",
    "        \n",
    "        with tf.device('/cpu:0'),tf.name_scope(\"embedding\"):\n",
    "            embeddings = tf.Variable(tf.truncated_normal([self.word_vocab_size,self.embedding_size],\n",
    "                                                              stddev=0.1),name = 'embedding_vocab')\n",
    "            embedding_diff_length = tf.Variable(tf.truncated_normal([80,20],\n",
    "                                                              stddev=0.01),name = 'embedding_length')\n",
    "        self.inputs1 = tf.nn.embedding_lookup(embeddings, self.input_s1)\n",
    "        self.inputs2 = tf.nn.embedding_lookup(embeddings, self.input_s2)\n",
    "        \n",
    "        self.lengths1 =tf.placeholder(tf.int32, [None], name='length1')\n",
    "        self.lengths2 =tf.placeholder(tf.int32, [None], name='length2')\n",
    "        \n",
    "        \n",
    "        self.difflength = tf.placeholder(tf.int32, [None], name='diff_length')\n",
    "        self.diff_len_embedding = tf.nn.embedding_lookup(embedding_diff_length, self.difflength)\n",
    "#         with tf.name_scope('gru_m2'):\n",
    "#             hidden_size_gru1 =300\n",
    "#             lstm_fw_cell = tf.contrib.rnn.GRUCell(hidden_size_gru1)\n",
    "#             lstm_bw_cell = tf.contrib.rnn.GRUCell(hidden_size_gru1)\n",
    "#             outputs_1, states_1  = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,\n",
    "#                                                                     cell_bw=lstm_bw_cell,\n",
    "#                                                                        dtype=tf.float32,\n",
    "#                                                                        sequence_length=length1,\n",
    "#                                                                        inputs=sentences1,\n",
    "#                                                                        scope = \"rnn1_m2\")\n",
    "#             state_fw1,state_bw1 = states_1\n",
    "#             state_cat1 = tf.concat([state_fw1,state_bw1],1)\n",
    "#             with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "#                 outputs_2, states_2  = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,\n",
    "#                                                                            cell_bw=lstm_bw_cell,\n",
    "#                                                                            dtype=tf.float32,\n",
    "#                                                                            sequence_length = length2,\n",
    "#                                                                            inputs=sentences2,\n",
    "#                                                                            scope = \"rnn1_m2\")\n",
    "#                 state_fw2,state_bw2 = states_2\n",
    "#                 state_cat2 = tf.concat([state_fw2,state_bw2],1)\n",
    "#             mul_state = tf.multiply(state_cat2,state_cat1)\n",
    "#             sub_state = tf.abs(tf.subtract(state_cat2,state_cat1))\n",
    "#             all_state = linear(tf.concat([mul_state,\n",
    "#                                           sub_state,\n",
    "#                                           state_cat2,\n",
    "#                                           state_cat1],axis= 1),600, scope='all_state_m2',stddev=0.1)\n",
    "        with tf.name_scope('lstm_m3'):\n",
    "            hidden_size =200\n",
    "            lstm_fw_cell1 = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "            lstm_bw_cell1 = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "            outputs_1, states_1  = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell1,\n",
    "                                                                   cell_bw=lstm_bw_cell1,\n",
    "                                                                   dtype=tf.float32,\n",
    "                                                                   sequence_length=self.lengths1,\n",
    "                                                                   inputs=self.inputs1,\n",
    "                                                                   scope = \"rnn_m3\")\n",
    "            output_fw_1,output_bw_1 = outputs_1\n",
    "            out_puts_1= tf.concat([output_fw_1, output_bw_1],2)\n",
    "            da = 100\n",
    "            r=100\n",
    "            out1 =tf.reshape(out_puts_1,[-1,hidden_size*2])\n",
    "            Ws1 = tf.Variable(tf.truncated_normal([hidden_size*2,da], stddev=0.1), name=\"Ws1\")\n",
    "            Ws2 = tf.Variable(tf.truncated_normal([da,r], stddev=0.1), name=\"Ws2\")\n",
    "            A1 = tf.reshape(tf.matmul(tf.tanh(tf.matmul(out1,Ws1)),Ws2),[-1,tf.shape(out_puts_1)[1],r])\n",
    "            transA1 = tf.nn.softmax(tf.transpose(A1,[0,2,1])) \n",
    "#             P1 = tf.matmul(self.transA1,tf.transpose(self.transA1,[0,2,1]))\n",
    "#             E1 = tf.eye(tf.shape(P1)[1],num_columns=tf.shape(P1)[2],batch_shape=[tf.shape(P1)[0]], dtype=tf.float32, name='E1')\n",
    "#             self.Reg_A1 = tf.nn.l2_loss(tf.subtract(P1,E1))\n",
    "            represent1 =tf.matmul(transA1,out_puts_1)\n",
    "            with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "                outputs_2, states_2  = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell1,\n",
    "                                                                       cell_bw=lstm_bw_cell1,\n",
    "                                                                       dtype=tf.float32,\n",
    "                                                                       sequence_length=self.lengths2,\n",
    "                                                                       inputs=self.inputs2,\n",
    "                                                                       scope = \"rnn_m3\")\n",
    "            \n",
    "          \n",
    "                output_fw_2,output_bw_2 = outputs_2\n",
    "                state_fw2,state_bw2 = states_2\n",
    "                out_puts_2 = tf.concat([output_fw_2, output_bw_2],2)\n",
    "                \n",
    "                out2 =tf.reshape(out_puts_2,[-1,hidden_size*2])\n",
    "                A2 = tf.reshape(tf.matmul(tf.tanh(tf.matmul(out2,Ws1)),Ws2),[-1,tf.shape(out_puts_2)[1],r])\n",
    "                transA2 = tf.nn.softmax(tf.transpose(A2,[0,2,1]))\n",
    "#                 P2 = tf.matmul(self.transA2,tf.transpose(self.transA2,[0,2,1]))\n",
    "#                 E2 = tf.eye(tf.shape(P2)[1],num_columns=tf.shape(P2)[2],batch_shape=[tf.shape(P2)[0]], dtype=tf.float32, name='E2')\n",
    "#                 self.Reg_A2 = tf.nn.l2_loss(tf.subtract(P2,E2))\n",
    "#                 self.reg = self.Reg_A1+self.Reg_A2\n",
    "                represent2 =tf.matmul(transA2,out_puts_2)\n",
    "#             self.mul_state = tf.multiply(self.represent1,self.represent2)\n",
    "#             self.sub_state = tf.abs(tf.subtract(self.represent1,self.represent2))\n",
    "#             self.state_re = linear(tf.concat([self.mul_state,self.sub_state],axis= 1), 100, scope='state_repre', stddev=0.1)\n",
    "            \n",
    "        mul_out2 = tf.multiply(represent2,represent1)\n",
    "        sub_out2 = tf.abs(tf.subtract(represent2,represent1))\n",
    "            \n",
    "        remul2 = tf.reshape(tf.nn.relu(mul_out2),[-1,r*hidden_size*2])\n",
    "        resub2 = tf.reshape(tf.nn.relu(sub_out2),[-1,r*hidden_size*2])\n",
    "            \n",
    "        represent1_resh = tf.reshape(tf.nn.relu(represent1),[-1,r*hidden_size*2])\n",
    "        represent2_resh = tf.reshape(tf.nn.relu(represent2),[-1,r*hidden_size*2])\n",
    "            \n",
    "        mul =linear(remul2, 100, scope='fc_mul', stddev=0.1)\n",
    "        sub =linear(resub2, 100, scope='fc_sub', stddev=0.1)\n",
    "        re1_h =linear(represent1_resh, 100, scope='fc_re', stddev=0.1)\n",
    "        with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "            re2_h =linear(represent2_resh, 100, scope='fc_re', stddev=0.1)\n",
    "            \n",
    "            \n",
    "        all_represent = tf.concat([mul,sub,re2_h,re1_h,self.diff_len_embedding],axis= 1)\n",
    "        h2 = linear(tf.nn.relu(all_represent), 1200, scope='fc_h2_m3', stddev=0.1)\n",
    "        with tf.name_scope(\"m3_prediction\"):\n",
    "            logits = linear(tf.nn.relu(h2),2,scope='m3_fc_out', stddev=0.1)\n",
    "            self.logits_m3 = tf.nn.softmax(logits,name= 'm3_out')\n",
    "        with tf.name_scope(\"m3_loss\"):\n",
    "            self.cross_entropy_m3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_onehot,\n",
    "                                                                                           logits=logits))\n",
    "        with tf.name_scope(\"m3_accuracy\"):\n",
    "            self.predictions_m3 = tf.argmax(self.logits_m3, 1, name=\"m3_predictions\")\n",
    "            correct_predictions = tf.equal(self.predictions_m3, tf.argmax(self.y_onehot, 1))\n",
    "            self.accuracy_m3 = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"m3_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_s1,batch_s2,batch_label,length1, length2= get_batch(np.array(data_index[0:64]))\n",
    "diff_length_Bat = np.abs(np.subtract(length1,length2))\n",
    "diff_length_Bat[diff_length_Bat>79]=79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ.setdefault('CUDA_VISIBLE_DEVICES','1')\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(gpu_options =gpu_options,allow_soft_placement=True,\n",
    "                                  log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        lstm= LSTM(word_vocab_size =vocabulary.__len__())\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        feed_dict = {\n",
    "            lstm.input_s1: batch_s1,\n",
    "            lstm.input_s2: batch_s2,\n",
    "            lstm.input_y: batch_label,\n",
    "            lstm.lengths1:length1,\n",
    "            lstm.lengths2:length2,\n",
    "            lstm.difflength:diff_length_Bat\n",
    "        }\n",
    "        predictions = sess.run(lstm.cross_entropy_m3,feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1+2+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.5+1+3.0/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_dev_size = 256\n",
    "dev_num = batch_dev_size*15\n",
    "train_data = data_index[:-dev_num]\n",
    "dev_data = data_index[-dev_num:]\n",
    "print('train_size: '+str(len(train_data))+' dev_size: '+str(len(dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def dev_validate(sess,step,saver,checkpoint_prefix,current_step):\n",
    "#     best_loss = 10\n",
    "#     all_dev_acc =[]\n",
    "#     acc_pred_d =[]\n",
    "#     pred_score = []\n",
    "#     true_dev_labels = []\n",
    "#     num_correct_pairs = 0\n",
    "#     for j in range(len(dev_data)/batch_dev_size):\n",
    "#         batch_s1_d,batch_s2_d,batch_label_d,length1_d, length2_d= get_batch(np.array(dev_data[j*batch_dev_size:(j+1)*batch_dev_size]))\n",
    "#         diff_length_Bat_dev = np.abs(np.subtract(length1_d,length2_d))\n",
    "#         diff_length_Bat_dev[diff_length_Bat_dev>79]=79\n",
    "#         feed_dict = {\n",
    "#             lstm.input_s1: batch_s1_d,\n",
    "#             lstm.input_s2: batch_s2_d,\n",
    "#             lstm.input_y: batch_label_d,\n",
    "#             lstm.lengths1:length1_d,\n",
    "#             lstm.lengths2:length2_d,\n",
    "#             lstm.difflength:diff_length_Bat_dev\n",
    "#         }\n",
    "#         acc_d,acc_pd,p_score = sess.run([lstm.accuracy_m3,\n",
    "#                                           lstm.predictions_m3,\n",
    "#                                           lstm.logits_m3],feed_dict)\n",
    "#         all_dev_acc.append(acc_d)\n",
    "#         acc_pred_d.append(acc_pd)\n",
    "#         pred_score.extend(p_score[:,1].astype(float))\n",
    "#         true_dev_labels.extend(batch_label_d)\n",
    "#         num_correct_pairs = num_correct_pairs+np.sum(np.equal(batch_label,np.argmax(pd).astype(int)))\n",
    "#     loss_dev = log_loss(true_dev_labels, pred_score)\n",
    "#     dev_acc = np.mean(all_dev_acc)\n",
    "#     print('step: '+str(step)+' acc: '+str(dev_acc) +'  '+str(sum(sum(acc_pred_d)))+ ' loss: '+str(loss_dev))\n",
    "#     if loss_dev < best_loss:\n",
    "#         best_loss = loss_dev\n",
    "#         path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "#     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "#     return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(trainData):\n",
    "    best_loss = 10\n",
    "    batch_size =64\n",
    "    epoches = 2\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "            allow_soft_placement=True,\n",
    "            log_device_placement=False)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            lstm= LSTM(word_vocab_size = vocabulary.__len__())\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optim = tf.train.AdamOptimizer(learning_rate=1e-3) \\\n",
    "            .minimize(lstm.cross_entropy_m3, global_step=global_step)\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(\n",
    "                os.path.curdir, \"quora_train\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            #save model\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.all_variables(),max_to_keep=10)\n",
    "            best_accuracy = 0\n",
    "            for epo in range(epoches):\n",
    "                np.random.shuffle(trainData)\n",
    "                for i in range(len(trainData)/batch_size):\n",
    "                    batch_s1,batch_s2,batch_label,length1, length2= get_batch(np.array(trainData[i*batch_size:(i+1)*batch_size]))\n",
    "                    current_step = tf.train.global_step(sess, global_step)\n",
    "                    diff_length_Bat = np.abs(np.subtract(length1,length2))\n",
    "                    diff_length_Bat[diff_length_Bat>79]=79\n",
    "                    feed_dict = {\n",
    "                        lstm.input_s1: batch_s1,\n",
    "                        lstm.input_s2: batch_s2,\n",
    "                        lstm.input_y: batch_label,\n",
    "                        lstm.lengths1:length1,\n",
    "                        lstm.lengths2:length2,\n",
    "                        lstm.difflength:diff_length_Bat\n",
    "                    }\n",
    "                    _ = sess.run(optim,feed_dict)\n",
    "                    if (i%50) ==0:\n",
    "                        all_dev_acc =[]\n",
    "                        acc_pred_d =[]\n",
    "                        pred_score = []\n",
    "                        true_dev_labels = []\n",
    "                        num_correct_pairs = 0\n",
    "                        for j in range(len(dev_data)/batch_dev_size):\n",
    "                            batch_s1_d,batch_s2_d,batch_label_d,length1_d, length2_d= get_batch(np.array(dev_data[j*batch_dev_size:(j+1)*batch_dev_size]))\n",
    "                            diff_length_Bat_dev = np.abs(np.subtract(length1_d,length2_d))\n",
    "                            diff_length_Bat_dev[diff_length_Bat_dev>79]=79\n",
    "                            feed_dict = {\n",
    "                                lstm.input_s1: batch_s1_d,\n",
    "                                lstm.input_s2: batch_s2_d,\n",
    "                                lstm.input_y: batch_label_d,\n",
    "                                lstm.lengths1:length1_d,\n",
    "                                lstm.lengths2:length2_d,\n",
    "                                lstm.difflength:diff_length_Bat_dev\n",
    "                            }\n",
    "                            acc_d,acc_pd,p_score = sess.run([lstm.accuracy_m3,\n",
    "                                                              lstm.predictions_m3,\n",
    "                                                              lstm.logits_m3],feed_dict)\n",
    "                            all_dev_acc.append(acc_d)\n",
    "                            acc_pred_d.append(acc_pd)\n",
    "                            pred_score.extend(p_score[:,1].astype(float))\n",
    "                            true_dev_labels.extend(batch_label_d)\n",
    "                            num_correct_pairs = num_correct_pairs+np.sum(np.equal(batch_label,np.argmax(pd).astype(int)))\n",
    "                        loss_dev = log_loss(true_dev_labels, pred_score)\n",
    "                        dev_acc = np.mean(all_dev_acc)\n",
    "                        print('step: '+str(i)+' acc: '+str(dev_acc) +'  '+str(sum(sum(acc_pred_d)))+ ' loss: '+str(loss_dev))\n",
    "                        if loss_dev < best_loss:\n",
    "                            best_loss = loss_dev\n",
    "                            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                        print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_last_model(last_model,data_last):\n",
    "    batch_t_size = 256\n",
    "    False_pairs = []\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "            allow_soft_placement=True,\n",
    "            log_device_placement=False)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            saver = tf.train.import_meta_graph(\"{}.meta\".format(last_model))\n",
    "            saver.restore(sess, last_model)\n",
    "            # Get the placeholders from the graph by name\n",
    "            input_t_s1 = graph.get_operation_by_name(\"input_placeholder_x1\").outputs[0]\n",
    "            input_t_s2 = graph.get_operation_by_name(\"input_placeholder_x2\").outputs[0]\n",
    "            lengtht_1 = graph.get_operation_by_name(\"length1\").outputs[0]\n",
    "            lengtht_2 = graph.get_operation_by_name(\"length2\").outputs[0]\n",
    "            diff_length = graph.get_operation_by_name(\"diff_length\").outputs[0]\n",
    "            predictions = graph.get_operation_by_name(\"m3_prediction/m3_out\").outputs[0]\n",
    "            pred_d =[]\n",
    "            for j in range(len(data_last)/batch_t_size):\n",
    "                batch_s1_d,batch_s2_d,batch_label,length1_d,length2_d= get_batch(np.array(data_last[j*batch_t_size:(j+1)*batch_t_size]),training =True)\n",
    "                diff_length_Bat = np.abs(np.subtract(length1_d,length2_d))\n",
    "                diff_length_Bat[diff_length_Bat>79]=79\n",
    "                feed_dict = {\n",
    "                    input_t_s1: batch_s1_d,\n",
    "                    input_t_s2: batch_s2_d,\n",
    "                    lengtht_1:length1_d,\n",
    "                    lengtht_2:length2_d,\n",
    "                    diff_length: diff_length_Bat\n",
    "                }\n",
    "                pd = sess.run(predictions,feed_dict)\n",
    "                index_False =np.reshape(np.argwhere(np.equal(batch_label,np.argmax(pd,axis=1))==False),[-1])\n",
    "                if len(index_False)!= 0:\n",
    "                    for id_f in index_False:\n",
    "                        False_pairs.append([batch_s1_d[id_f][:length1_d[id_f]].tolist(),\n",
    "                                            batch_s2_d[id_f][:length2_d[id_f]].tolist(),\n",
    "                                            batch_label[id_f]])\n",
    "            if len(False_pairs)!= 0:\n",
    "                print('false_pairs_length: ' + str(len(False_pairs)))\n",
    "    return False_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import log_loss\n",
    "num_base_clfs=10\n",
    "ensem_files =[]\n",
    "ensem_files.append('/home/zhangjinbin/research/quora/quora_train/1493780013/checkpoints/model-11994')\n",
    "num_c =0\n",
    "num_resample = 1\n",
    "while num_c < num_base_clfs:\n",
    "    false_pairs = test_last_model(ensem_files[-1],train_data)\n",
    "    resample_pairs = []\n",
    "    for _ in range(num_resample):\n",
    "        resample_pairs.extend(false_pairs)\n",
    "    resample_pairs.extend(train_data)\n",
    "    num_c+=1\n",
    "    best_model_epo = train_model(resample_pairs)\n",
    "    ensem_files.append(best_model_epo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
