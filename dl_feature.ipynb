{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/train_clean.csv')\n",
    "df_test = pd.read_csv('./data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_att = 'q1_stems_no_stops'\n",
    "q2_att = 'q2_stems_no_stops'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_1=[]\n",
    "texts_2 =[]\n",
    "labels = []\n",
    "for q1,q2,is_dup in zip(df_train[q1_att],df_train[q2_att],df_train.is_duplicate):\n",
    "    texts_1.append(str(q1).split())\n",
    "    texts_2.append(str(q2).split())\n",
    "    labels.append(int(is_dup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_texts_1 =[]\n",
    "test_texts_2 =[]\n",
    "test_ids = []\n",
    "for q1,q2,id_ in zip(df_test[q1_att],df_test[q2_att],df_test.test_id):\n",
    "    test_texts_1.append(str(q1).split())\n",
    "    test_texts_2.append(str(q2).split())\n",
    "    test_ids.append(id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_train,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import itertools\n",
    "def build_vocab(sentences, max_vocab=30000):\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    vocabulary_inv = []\n",
    "    vocabulary_inv.append(\"<PAD/>\")\n",
    "    vocabulary_inv.append(\"<mino/>\")\n",
    "    vocabulary_inv.extend([x[0] for x in word_counts.most_common(max_vocab)])\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocabulary, vocabulary_inv = build_vocab(test_texts_1+test_texts_2+texts_1+texts_2, max_vocab=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('./data/embedding/vacabulary_quora_stemd.pkl', 'wb')\n",
    "pickle.dump(vocabulary,output)\n",
    "output.close()\n",
    "# output = open('./processed_data/vocabulary_inv_quora_cle.pkl', 'wb') \n",
    "# pickle.dump(vocabulary,output)\n",
    "# output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/embedding/vacabulary_quora_cle.pkl', 'r') as output:\n",
    "    vocabulary = pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data(ques1,ques2,label,vocabulary):\n",
    "    mino_idx = vocabulary[\"<mino/>\"]\n",
    "    data_index = []\n",
    "    for q1,q2,dup in zip(ques1,ques2,label):\n",
    "        sentence1 = [vocabulary.get(word, mino_idx) for word in q1]\n",
    "        sentence2 = [vocabulary.get(word, mino_idx) for word in q2]\n",
    "        data_index.append([sentence1,sentence2,dup])\n",
    "    return data_index\n",
    "data_index = build_data(texts_1,texts_2,labels,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_test_data(ques1,ques2,vocabulary):\n",
    "    mino_idx = vocabulary[\"<mino/>\"]\n",
    "    data_index = []\n",
    "    for q1,q2 in zip(ques1,ques2):\n",
    "        sentence1 = [vocabulary.get(word, mino_idx) for word in q1]\n",
    "        sentence2 = [vocabulary.get(word, mino_idx) for word in q2]\n",
    "        data_index.append([sentence1,sentence2])\n",
    "    return data_index\n",
    "test_data_index = build_test_data(test_texts_1,test_texts_2,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del texts_1,texts_2,test_texts_1,test_texts_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "EMBEDDING_FILE = './data/embedding/GoogleNews-vectors-negative300.bin'\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE,binary=True)\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(MAX_NB_WORDS, len(vocabulary))+1\n",
    "embedding_matrix = np.random.normal(loc=0,scale=0.1,size=[nb_words, EMBEDDING_DIM])\n",
    "for word, i in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodl_train_fea1 = pd.read_csv('./data/train_no_dl_fea1.csv',usecols=['k_q1_q2_intersect',\n",
    "       'k_q1_freq', 'k_q2_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodl_test_fea1 = pd.read_csv('./data/test_no_dl_fea1.csv',usecols=['k_q1_q2_intersect',\n",
    "       'k_q1_freq', 'k_q2_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leaks = nodl_train_fea1[['k_q1_q2_intersect','k_q1_freq', 'k_q2_freq']]\n",
    "test_leaks = nodl_test_fea1[['k_q1_q2_intersect','k_q1_freq', 'k_q2_freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leaks, test_leaks)))\n",
    "train_leaks_fea = ss.transform(leaks)\n",
    "test_leaks_fea = ss.transform(test_leaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Creating the model\n",
    "word2vec = KeyedVectors.load_word2vec_format('./data/embedding/wiki.en.vec')\n",
    "\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(MAX_NB_WORDS, len(vocabulary))+1\n",
    "embedding_matrix = np.random.normal(loc=0,scale=0.1,size=[nb_words, EMBEDDING_DIM])\n",
    "for word, i in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(MAX_NB_WORDS, len(vocabulary))+1\n",
    "embeddings_index = {}\n",
    "f = open('./data/embedding/glove.6B.300d.txt')\n",
    "count = 0\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "embedding_matrix = np.random.normal(loc=0,scale=0.1,size=[nb_words, EMBEDDING_DIM])\n",
    "for word, i in vocabulary.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(embedding_matrix)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class biRNN(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size,hidden_size,output_size,n_layers = 1):\n",
    "        super(biRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size =output_size\n",
    "        self.embed= nn.Embedding(vocab_size,embedding_size)\n",
    "        self.gru= nn.GRU(input_size=self.embedding_size,\n",
    "                            hidden_size=self.hidden_size,\n",
    "                            num_layers = self.n_layers,\n",
    "                            batch_first =True,\n",
    "                            bidirectional = True)\n",
    "        self.da = 100\n",
    "        self.r = 100\n",
    "        self.fc1 = nn.Linear(100*4+100, 1200)\n",
    "        self.fc2 = nn.Linear(1200, 2)\n",
    "        self.fc_leak = nn.Linear(3,100)\n",
    "        self.fcWs1 = nn.Linear(self.hidden_size*2, self.da)\n",
    "        self.fcWs2 = nn.Linear(self.da,self.r)\n",
    "        self.fc_dedim_mul = nn.Linear(self.hidden_size*2*self.r,100)\n",
    "        self.fc_dedim_sub = nn.Linear(self.hidden_size*2*self.r,100)\n",
    "        self.fc_dedim_rep = nn.Linear(self.hidden_size*2*self.r,100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "    def forward(self,leak_input,sentence1,sentence2,length1,length2,pad1_V,pad2_V,cuda_available = False):\n",
    "        \n",
    "#         h1 = torch.mean(self.embed(sentence1),1)\n",
    "#         h1 =torch.squeeze(h1, dim=1)\n",
    "#         h2 = torch.mean(self.embed(sentence2),1)\n",
    "#         h2 =torch.squeeze(h2,dim=1)\n",
    "\n",
    "        input_1 = self.embed(sentence1)\n",
    "        input_2 = self.embed(sentence2)\n",
    "        if cuda_available:\n",
    "            output_1 = torch.mul(pad1_V.float().cuda(),input_1)\n",
    "            output_2 = torch.mul(pad2_V.float().cuda(),input_2)\n",
    "        else:\n",
    "            output_1 = torch.mul(pad1_V.float(),input_1)\n",
    "            output_2 = torch.mul(pad2_V.float(),input_2)\n",
    "        output_1,hidden1 = self.gru(output_1)\n",
    "        \n",
    "        output_2,hidden2 = self.gru(output_2)\n",
    "#         output_1,hidden1 = self.gru(self.embed(sentence1))\n",
    "        \n",
    "#         output_2,hidden2 = self.gru(self.embed(sentence2))\n",
    "#         if cuda_available:\n",
    "#             output_1 = torch.mul(pad1_V.float().cuda(),output_1)\n",
    "#             output_2 = torch.mul(pad2_V.float().cuda(),output_2)\n",
    "#         else:\n",
    "#             output_1 = torch.mul(pad1_V.float(),output_1)\n",
    "#             output_2 = torch.mul(pad2_V.float(),output_2)\n",
    "#         if cuda_available:\n",
    "#             output_1_mask = Variable(torch.zeros(output_1.size())).cuda()\n",
    "#             output_2_mask = Variable(torch.zeros(output_2.size())).cuda()\n",
    "#         else:\n",
    "#             output_1_mask = Variable(torch.zeros(output_1.size()))\n",
    "#             output_2_mask = Variable(torch.zeros(output_2.size()))\n",
    "#         for i in range(len(length1)):\n",
    "# #             output1[i] = torch.cat((output_1[i,length1[i]-1,:self.hidden_size],output_1[i,0,-self.hidden_size:]), 0)\n",
    "# #             output2[i] = torch.cat((output_2[i,length2[i]-1,:self.hidden_size],output_2[i,0,-self.hidden_size:]), 0)\n",
    "#             A1[i] = torch.mm(self.softmax(torch.t(self.fcWs2(torch.tanh(self.fcWs1(output_1[i,:length1[i],:]))))),\n",
    "#                                   output_1[i,:length1[i],:])\n",
    "#             A2[i] = torch.mm(self.softmax(torch.t(self.fcWs2(torch.tanh(self.fcWs1(output_2[i,:length2[i],:]))))),\n",
    "#                                   output_2[i,:length2[i],:])\n",
    "        A1 = self.fcWs2(torch.tanh(self.fcWs1(output_1.contiguous().view(-1,self.hidden_size*2)))).view(output_1.size()[0],\n",
    "                                                                                                        output_1.size()[1],\n",
    "                                                                                                        -1)\n",
    "        A1_t = self.softmax(torch.transpose(A1, 1, 2).contiguous().view(-1,output_1.size()[1])).view(output_1.size()[0],-1,output_1.size()[1])\n",
    "        output1_new = torch.bmm(A1_t, output_1)\n",
    "        \n",
    "        A2 = self.fcWs2(torch.tanh(self.fcWs1(output_2.contiguous().view(-1,self.hidden_size*2)))).view(output_2.size()[0],\n",
    "                                                                                                        output_2.size()[1],\n",
    "                                                                                                        -1)\n",
    "        A2_t = self.softmax(torch.transpose(A2, 1, 2).contiguous().view(-1,output_2.size()[1])).view(output_2.size()[0],\n",
    "                                                                                                     -1,\n",
    "                                                                                                     output_2.size()[1])\n",
    "        output2_new = torch.bmm(A2_t, output_2)\n",
    "    #         A2[i] = torch.mm(self.softmax(torch.t(self.fcWs2(torch.tanh(self.fcWs1(output_2[i,:length2[i],:]))))),\n",
    "#                                   output_2[i,:length2[i],:])\n",
    "        output1_new = output1_new.view(-1,self.r*self.hidden_size*2)\n",
    "        output2_new = output2_new.view(-1,self.r*self.hidden_size*2)\n",
    "        mul_represent = torch.mul(output1_new,output2_new)\n",
    "        sub_represent = torch.abs(output1_new-output2_new)\n",
    "        mul_dedim = self.fc_dedim_mul(mul_represent)\n",
    "        sub_dedim = self.fc_dedim_sub(sub_represent)\n",
    "        rep1_dedim = self.fc_dedim_rep(output1_new)\n",
    "        rep2_dedim = self.fc_dedim_rep(output2_new)\n",
    "#         mul_represent = torch.mul(rep1_dedim,rep2_dedim)\n",
    "#         sub_represent = torch.abs(rep1_dedim-rep2_dedim)\n",
    "        dense_leak = self.fc_leak(leak_input)\n",
    "        out_mul = self.relu(torch.cat((mul_dedim,sub_dedim,rep1_dedim,rep2_dedim,dense_leak),1))\n",
    "        logits  = self.fc2(self.relu(self.fc1(out_mul)))\n",
    "        return logits\n",
    "    def init_weights(self,pretrained_weight):\n",
    "        #init(self.embed.weight.data)\n",
    "        initrange = 0.001\n",
    "#         nn.init.uniform(self.embedding,a=0,b=0.00\n",
    "        self.fc1.bias.data.fill_(0)\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        #torch.nn.init.xavier_uniform(self.fc.weight.data, gain=math.sqrt(2.0))\n",
    "        #self.embed.weight.data.normal_(mean=0, std=0.01)\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.fill_(0)\n",
    "        \n",
    "        self.fcWs1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fcWs1.bias.data.fill_(0)\n",
    "        self.fcWs2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fcWs2.bias.data.fill_(0)\n",
    "    def initHidden(self,batch_size):\n",
    "        h0 = Variable(torch.zeros(2*self.n_layers,batch_size,self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(2*self.n_layers,batch_size,self.hidden_size))\n",
    "        return h0,c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class biRNN(nn.Module):\n",
    "#     def __init__(self,vocab_size,embedding_size,hidden_size,output_size,n_layers = 1):\n",
    "#         super(biRNN, self).__init__()\n",
    "#         self.n_layers = n_layers\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.embedding_size = embedding_size\n",
    "#         self.output_size =output_size\n",
    "#         self.embed = nn.Embedding(vocab_size,embedding_size)\n",
    "#         self.gru = nn.GRU(input_size=self.embedding_size,\n",
    "#                             hidden_size=self.hidden_size,\n",
    "#                             num_layers = self.n_layers,\n",
    "#                             batch_first =True,\n",
    "#                             bidirectional = True)\n",
    "#         self.lstm = nn.LSTM(input_size=self.embedding_size,\n",
    "#                             hidden_size=self.hidden_size,\n",
    "#                             num_layers = self.n_layers,\n",
    "#                             batch_first =True,\n",
    "#                             bidirectional = True)\n",
    "#         #self.BN0 = nn.BatchNorm1d(2400)\n",
    "#         self.fc1 = nn.Linear(4*self.hidden_size*2*self.n_layers+100, 1200)\n",
    "#         #self.BN1 = nn.BatchNorm1d(1200)\n",
    "#         self.fc2 = nn.Linear(1200, 2)\n",
    "#         self.fc_leak = nn.Linear(3, 100)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.softmax = nn.Softmax()\n",
    "#     def forward(self,leak_input,sentence1,sentence2,length1,length2,pad1_V,pad2_V,cuda_available = False):\n",
    "# # \n",
    "#         input_1 = self.embed(sentence1)\n",
    "#         input_2 = self.embed(sentence2)\n",
    "# #         if cuda_available:\n",
    "# #             output_1 = torch.mul(pad1_V.float().cuda(),input_1)\n",
    "# #             output_2 = torch.mul(pad2_V.float().cuda(),input_2)\n",
    "# #         else:\n",
    "# #             output_1 = torch.mul(pad1_V.float(),input_1)\n",
    "# #             output_2 = torch.mul(pad2_V.float(),input_2)\n",
    "#         output_1,hidden1 = self.gru(input_1)\n",
    "#         output_2,hidden2 = self.gru(input_2)\n",
    "        \n",
    "# #         ls_o1,(ls_h1,ls_c1) = self.lstm(input_1)\n",
    "# #         ls_o2,(ls_h2,ls_c2) = self.lstm(input_2)\n",
    "        \n",
    "#         A = torch.transpose(hidden1, 0, 1)\n",
    "#         B = torch.transpose(hidden2, 0, 1)\n",
    "#         A = A.contiguous().view(input_1.size()[0],-1)\n",
    "#         B = B.contiguous().view(input_2.size()[0],-1)\n",
    "        \n",
    "# #         A2 = torch.transpose(ls_h1, 0, 1)\n",
    "# #         B2 = torch.transpose(ls_h2, 0, 1)\n",
    "# #         A2 = A2.contiguous().view(input_1.size()[0],-1)\n",
    "# #         B2 = B2.contiguous().view(input_2.size()[0],-1)\n",
    "# #         output1 = torch.cat((output_1[:,length1,:self.hidden_size],output_1[i,0,-self.hidden_size:]), 0)\n",
    "# #         output2 = torch.cat((output_2[:,length2,:self.hidden_size],output_2[i,0,-self.hidden_size:]), 0)\n",
    "# #         A = torch.cat((A1,A2), 1)\n",
    "# #         B = torch.cat((B1,B2), 1)\n",
    "#         mul_represent = torch.mul(A,B)\n",
    "#         sub_represent = torch.abs(A-B)\n",
    "#         dense_leak = self.fc_leak(leak_input)\n",
    "#         out_mul = self.relu(torch.cat((mul_represent,sub_represent,A,B,dense_leak),1))\n",
    "#         logits  = self.fc2(self.relu(self.fc1(out_mul)))\n",
    "#         return logits\n",
    "#     def init_weights(self,pretrained_weight):\n",
    "#         #init(self.embed.weight.data)\n",
    "#         initrange = 0.001\n",
    "# #         nn.init.uniform(self.embedding,a=0,b=0.00\n",
    "#         self.fc1.bias.data.fill_(0)\n",
    "#         self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "#         #torch.nn.init.xavier_uniform(self.fc.weight.data, gain=math.sqrt(2.0))\n",
    "#         #self.embed.weight.data.normal_(mean=0, std=0.01)\n",
    "#         self.embed.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "#         self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.fc2.bias.data.fill_(0)\n",
    "#     def initHidden(self,batch_size):\n",
    "#         h0 = Variable(torch.zeros(2*self.n_layers,batch_size,self.hidden_size))\n",
    "#         c0 = Variable(torch.zeros(2*self.n_layers,batch_size,self.hidden_size))\n",
    "#         return h0,c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bilstm = biRNN(vocab_size = np.shape(embedding_matrix)[0],embedding_size = np.shape(embedding_matrix)[1],hidden_size = 300,output_size =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bilstm.init_weights(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(batch,hidden_num = 100,training = True):\n",
    "    length1 = [len(sen1) for sen1 in batch[:,0]]\n",
    "    length2 = [len(sen2) for sen2 in batch[:,1]]\n",
    "    max_length1 = max(length1)\n",
    "    max_length2 = max(length2)\n",
    "    s1_batch = []\n",
    "    s2_batch = []\n",
    "    s1_batch_pad = []\n",
    "    s2_batch_pad = []\n",
    "    for sen in batch:\n",
    "        if len(sen[0])==max_length1:\n",
    "            s1_batch.append(sen[0])\n",
    "            s1_batch_pad.append(np.concatenate((np.ones([len(sen[0]),hidden_num]),\n",
    "                                                np.zeros([max_length1-len(sen[0]),hidden_num])),0))\n",
    "        else:\n",
    "            s1_batch.append(sen[0]+(max_length1-len(sen[0]))*[0])\n",
    "            s1_batch_pad.append(np.concatenate((np.ones([len(sen[0]),hidden_num]),\n",
    "                                                np.zeros([max_length1-len(sen[0]),hidden_num])),0))\n",
    "        if len(sen[1])==max_length2:\n",
    "            s2_batch.append(sen[1])\n",
    "            s2_batch_pad.append(np.concatenate((np.ones([len(sen[1]),hidden_num]),\n",
    "                                                np.zeros([max_length2-len(sen[1]),hidden_num])),0))\n",
    "        else:\n",
    "            s2_batch.append(sen[1]+(max_length2-len(sen[1]))*[0])\n",
    "            s2_batch_pad.append(np.concatenate((np.ones([len(sen[1]),hidden_num]),\n",
    "                                                np.zeros([max_length2-len(sen[1]),hidden_num])),0))\n",
    "    if training:   \n",
    "        return [Variable(torch.from_numpy(np.array(s1_batch))),\n",
    "                Variable(torch.from_numpy(np.array(s2_batch))),\n",
    "                batch[:,2].astype(int),length1,length2,\n",
    "                Variable(torch.from_numpy(np.array(s1_batch_pad))),\n",
    "                Variable(torch.from_numpy(np.array(s2_batch_pad)))]\n",
    "    else:\n",
    "        return [Variable(torch.from_numpy(np.array(s1_batch))),\n",
    "                Variable(torch.from_numpy(np.array(s2_batch))),\n",
    "                length1,length2,\n",
    "                Variable(torch.from_numpy(np.array(s1_batch_pad))),\n",
    "                Variable(torch.from_numpy(np.array(s2_batch_pad)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_s1,batch_s2,batch_label,length1_b,length2_b ,pad1,pad2= get_batch(np.array(data_index[0:10]),hidden_num=300,training=True)\n",
    "leak_in = Variable(torch.from_numpy(np.array(train_leaks_fea[0:10]))).float()\n",
    "o = bilstm(leak_in,batch_s1,batch_s2,length1_b,length2_b,pad1,pad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device_id = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "criterior_cross = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bilstm.parameters(), lr=1e-3)\n",
    "criterior_cross.cuda()\n",
    "bilstm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(model_path,trainD,leak_te):\n",
    "    trainData = trainD.tolist()\n",
    "    test_leak_data = leak_te.tolist()\n",
    "    print('len train    '+str(len(trainData)))\n",
    "    torch_softmax= nn.Softmax()\n",
    "    bilstm_test = biRNN(vocab_size = np.shape(embedding_matrix)[0],\n",
    "                        embedding_size = np.shape(embedding_matrix)[1],\n",
    "                        hidden_size = 300,output_size =2)\n",
    "    bilstm_test.load_state_dict(torch.load(model_path))\n",
    "    torch.cuda.set_device(device_id)\n",
    "    bilstm_test.cuda()\n",
    "    batch_t_size = 128\n",
    "    pad_num_train = (len(trainData)/batch_t_size+1)*batch_t_size-len(trainData)\n",
    "    for i in range(pad_num_train):\n",
    "        trainData.append([[0,0],[0,0],1])\n",
    "        test_leak_data.append([0,0,0])\n",
    "    pred_train = []\n",
    "    for i in range(len(trainData)/batch_t_size):\n",
    "        batcht_s1,batcht_s2,batcht_labels,lengtht_s1,lengtht_s2,padt1,padt2 = get_batch(np.array(trainData[i*batch_t_size:(i+1)*batch_t_size]),\n",
    "                                                                                 hidden_num=300,training=True)\n",
    "        leak_input = Variable(torch.from_numpy(np.array(test_leak_data[i*batch_t_size:(i+1)*batch_t_size]))).float()\n",
    "        \n",
    "        o = bilstm_test(leak_input.cuda(),batcht_s1.cuda(),batcht_s2.cuda(),lengtht_s1,lengtht_s2,padt1,padt2,cuda_available=True)\n",
    "        out_model = torch_softmax(o)\n",
    "        pred_train.extend(out_model.data.cpu().numpy()[:,1].astype(float))\n",
    "        if i%200==0:\n",
    "            print(\"train step: \"+str(i))\n",
    "    print('len train pred   '+str(len(pred_train[:-pad_num_train])))\n",
    "    del trainData\n",
    "    return pred_train[:-pad_num_train]\n",
    "\n",
    "def train_model(train_d,dev_d,train_leak,dev_leak,id_m,name_model):\n",
    "    trainD = np.array(train_d).copy()\n",
    "    devdata = np.array(dev_d).copy()\n",
    "    leak_t_D = np.array(train_leak).copy()\n",
    "    leak_dev_data = np.array(dev_leak).copy()\n",
    "    batch_size =64\n",
    "    best_acc = 0\n",
    "    epoches = 1\n",
    "    best_loss = 10\n",
    "    batch_dev_size =128\n",
    "    torch_softmax= nn.Softmax()\n",
    "    bilstm_tr = biRNN(vocab_size = np.shape(embedding_matrix)[0],\n",
    "                   embedding_size = np.shape(embedding_matrix)[1],\n",
    "                   hidden_size = 300,\n",
    "                   output_size =2)\n",
    "    bilstm_tr.init_weights(embedding_matrix)\n",
    "    crossEn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_tr = torch.optim.Adam(bilstm_tr.parameters(), lr=1e-3)\n",
    "    torch.cuda.set_device(device_id)\n",
    "    crossEn.cuda()\n",
    "    bilstm_tr.cuda()\n",
    "    from sklearn.metrics import log_loss\n",
    "    path = './model_stack/'+name_model+'0_'+str(id_m)+'.pkl'\n",
    "    for epoch in range(epoches):\n",
    "        #perm = np.random.permutation(len(trainD))\n",
    "        trainData = trainD\n",
    "        leakdata = leak_t_D\n",
    "        if epoch==0:\n",
    "            check_steps=250\n",
    "        else:\n",
    "            check_steps=100\n",
    "        for i in range(len(trainData)/batch_size):\n",
    "            optimizer_tr.zero_grad()\n",
    "            batch_s1,batch_s2,batch_labels,length_s1,length_s2,pad1,pad2 = get_batch(np.array(trainData[i*batch_size:(i+1)*batch_size]),\n",
    "                                                                                     hidden_num=300,training=True)\n",
    "            leak_input = Variable(torch.from_numpy(np.array(leakdata[i*batch_size:(i+1)*batch_size]))).float()\n",
    "            if torch.cuda.is_available():\n",
    "                o_tr = bilstm_tr(leak_input.cuda(),batch_s1.cuda(),batch_s2.cuda(),length_s1,length_s2,pad1,pad2,cuda_available=True)  \n",
    "            loss_ = crossEn(o_tr, Variable(torch.from_numpy(batch_labels)).cuda())\n",
    "            loss_.backward()\n",
    "            optimizer_tr.step()\n",
    "            if (i+1) % check_steps ==0:\n",
    "                count_correct =0\n",
    "                allsum =[]\n",
    "                pred_dev_batch = []\n",
    "                true_labels_dev = []\n",
    "                for num in range(len(devdata)/batch_dev_size):\n",
    "                    batch_s1_dev,batch_s2_dev,batch_labels_dev,length_s1_dev,length_s2_dev,pad1dev,pad2dev = get_batch(np.array(devdata[num*batch_dev_size:(num+1)*batch_dev_size]),\n",
    "                                                                                                                       hidden_num=300)\n",
    "                    leak_dev_input = Variable(torch.from_numpy(np.array(leak_dev_data[num*batch_dev_size:(num+1)*batch_dev_size]))).float()\n",
    "                    od = bilstm_tr(leak_dev_input.cuda(),batch_s1_dev.cuda(),batch_s2_dev.cuda(),length_s1_dev,length_s2_dev,pad1dev,pad2dev,cuda_available=True)\n",
    "                    out_model = torch_softmax(od)\n",
    "                    allsum.append(sum(out_model.data.max(1)[1].view(-1).cpu()))\n",
    "                    count_correct += sum(out_model.data.max(1)[1].view(-1).cpu().numpy()==batch_labels_dev)#.cuda().long()).long().sum().data[0]\n",
    "                    pred_dev_batch.extend(out_model.data.cpu().numpy()[:,1].astype(float))\n",
    "                    #bi_loss = binary_log_loss(o.data[:,1],torch.from_numpy(batch_labels_dev).cuda())\n",
    "                    true_labels_dev.extend(batch_labels_dev)\n",
    "                    #print(bi_loss)\n",
    "                acc = count_correct/(len(devdata)+0.0)\n",
    "                current_loss = log_loss(true_labels_dev, pred_dev_batch)\n",
    "#                 if current_loss<best_loss:\n",
    "#                     best_loss = current_loss\n",
    "#                     torch.save(bilstm_tr.state_dict(), path)\n",
    "#                     print(\"save in : \"+ path)\n",
    "# #                     step = step +1\n",
    "# #                     if step==num_model:\n",
    "# #                         step = 0\n",
    "                print('step: '+str(i) +' acc: '+str(acc)+' '+ str(sum(allsum))+\" loss: \"+str(current_loss))\n",
    "    torch.save(bilstm_tr.state_dict(), path)\n",
    "    print('save in : '+ path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/kdf.pkl', 'r') as f:\n",
    "    kf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# skf = StratifiedKFold(n_splits=8,random_state=0)\n",
    "splits = []\n",
    "train_ids = []\n",
    "for (train_index, val_index) in kf:\n",
    "    splits.append([train_index,val_index])\n",
    "    train_ids.extend(val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_ = 'stems_no_stops_self_att_glove'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "train_feature = []\n",
    "test_feature = []\n",
    "for num_c,spl in enumerate(splits):\n",
    "    train_data,val_data = np.array(data_index)[spl[0]],np.array(data_index)[spl[1]]\n",
    "    train_leaks,dev_leaks = np.array(train_leaks_fea)[spl[0]],np.array(train_leaks_fea)[spl[1]]\n",
    "    best_model_epo = train_model(train_data,val_data[0:4000],train_leaks,dev_leaks[0:4000],num_c,name_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#device_id =2\n",
    "train_feature = []\n",
    "for num_c,spl in enumerate(splits):\n",
    "    path = './model_stack/'+name_+'0_'+str(num_c)+'.pkl'\n",
    "    val_data = np.array(data_index)[spl[1]]\n",
    "    dev_leaks = np.array(train_leaks_fea)[spl[1]]\n",
    "    train_pred = test_model(path,val_data,dev_leaks)\n",
    "    train_feature.extend(train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid = np.array(data_index)[:-400],np.array(data_index)[-400:]\n",
    "leak_train_new_x,leak_valid_new_x = np.array(train_leaks_fea)[:-400],np.array(train_leaks_fea)[-400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_for_test = train_model(x_train,x_valid,leak_train_new_x,leak_valid_new_x,100,name_+'for_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_(model_path,testD,leak_test):\n",
    "    testData = testD.tolist()\n",
    "    leak_t = leak_test.tolist()\n",
    "    print('len test  '+str(len(testData)))\n",
    "    torch_softmax= nn.Softmax()\n",
    "    bilstm_test = biRNN(vocab_size = np.shape(embedding_matrix)[0],\n",
    "                        embedding_size = np.shape(embedding_matrix)[1],\n",
    "                        hidden_size = 300,output_size =2)\n",
    "    bilstm_test.load_state_dict(torch.load(model_path))\n",
    "    torch.cuda.set_device(device_id)\n",
    "    bilstm_test.cuda()\n",
    "    batch_t_size = 128\n",
    "    pad_num_test = (len(testData)/batch_t_size+1)*batch_t_size-len(testData)\n",
    "    for i in range(pad_num_test):\n",
    "        testData.append([[0,0],[0,0]])\n",
    "        leak_t.append([0,0,0])\n",
    "    pred_test = []\n",
    "    for i in range(len(testData)/batch_t_size):\n",
    "        batcht_s1,batcht_s2,lengtht_s1,lengtht_s2,padt1,padt2 = get_batch(np.array(testData[i*batch_t_size:(i+1)*batch_t_size]),\n",
    "                                                                                 hidden_num=300,training=False)\n",
    "        leak_input = Variable(torch.from_numpy(np.array(leak_t[i*batch_t_size:(i+1)*batch_t_size]))).float()\n",
    "        o = bilstm_test(leak_input.cuda(),batcht_s1.cuda(),batcht_s2.cuda(),lengtht_s1,lengtht_s2,padt1,padt2,cuda_available=True)\n",
    "        out_model = torch_softmax(o)\n",
    "        pred_test.extend(out_model.data.cpu().numpy()[:,1].astype(float))\n",
    "        if i%200==0:\n",
    "            print(\"test step: \"+str(i))      \n",
    "    print('len test pred  '+str(len(pred_test[:-pad_num_test])))\n",
    "    del testData\n",
    "    return pred_test[:-pad_num_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_pred = test_(model_for_test,np.array(test_data_index),np.array(test_leaks_fea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_loss(np.array(labels)[train_ids],train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dl = np.zeros(len(train_ids))\n",
    "for i,id_ in enumerate(train_ids):\n",
    "    train_dl[id_] = train_feature[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindl_file = pd.read_csv('./stack_dl_feature/train_dl_with_leak.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindl_file[name_] = train_dl\n",
    "traindl_file.to_csv('./stack_dl_feature/train_dl_with_leak.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdl_file = pd.read_csv('./stack_dl_feature/test_dl_with_leak.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdl_file[name_] = test_pred\n",
    "testdl_file.to_csv('./stack_dl_feature/test_dl_with_leak.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stdmean_file= pd.read_csv('./stack_dl_feature/stdmean_file.csv')\n",
    "std = np.std(train_dl)\n",
    "mean = np.mean(train_dl)\n",
    "print mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stdmean_file[name_] = [mean,std]\n",
    "stdmean_file.to_csv('./stack_dl_feature/stdmean_file.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindl_file = pd.read_csv('./stack_dl_feature/dl_fea_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindl_file[name_] = train_dl\n",
    "traindl_file.to_csv('./stack_dl_feature/dl_fea_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdl_file = pd.read_csv('./stack_dl_feature/dl_fea_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pred_dl_f = np.mean(test_feature,axis=0)\n",
    "testdl_file[name_] = test_pred\n",
    "testdl_file.to_csv('./stack_dl_feature/dl_fea_test.csv',index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
