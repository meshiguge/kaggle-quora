{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "file_pred=open('./data/querry_pred.csv', \"r\") # the stacknet prediction file\n",
    "counter=0\n",
    "for line in file_pred:\n",
    "    splits=line.replace(\"\\n\",\"\").split(\",\") \n",
    "    #print to the new file\n",
    "    pred.append(float(splits[1]))\n",
    "    counter+=1\n",
    "    if counter%100000==0:\n",
    "        print( \" printing row %d \" % (counter))\n",
    "file_pred.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodl_train_fea = pd.read_csv('./data/train_no_dl_fea.csv')\n",
    "nodl_train_fea1 = pd.read_csv('./data/train_no_dl_fea1.csv')\n",
    "# dl_train_fea = pd.read_csv('./data/dl_train.csv')\n",
    "# dl_train_fea1 = pd.read_csv('./data/dl_train_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dl_train_fea2 = pd.read_csv('./stack_dl_feature/dl_fea_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train_org.csv',usecols=['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = [c for c in nodl_train_fea1.columns if ((c[:2]=='h_')or(c[:2]=='k_'))]+['w_porter_interaction',\n",
    " 'w_porter_jaccard',\n",
    " 'w_porter_levenshtein_1',\n",
    " 'w_porter_levenshtein_2',\n",
    " 'w_porter_sorensen']#\n",
    "for c in col:\n",
    "    nodl_train_fea[c] = nodl_train_fea1[c]\n",
    "# for c in ['no_stops_dl', u'clean_dl', u'no_stops_birnn_dl',\n",
    "#        u'clean_dl_birnn', u'no_stops_birnn_dl_difference',\n",
    "#        u'clean_birnn_dl_difference', u'stemd_birnn',\n",
    "#        u'no_stops_birnn_dl_glove', u'clean_difference_dl_glove',\n",
    "#        u'clean_dl_birnn_glove', u'adj_n_v_nostopsdl_glove',\n",
    "#        u'stemd_birnn_glove', u'stems_birnn_glove', u'stems_birnn_google']:#\n",
    "#     nodl_train_fea[c] = dl_train_fea[c]\n",
    "# nodl_train_fea['noun_verb_adj_birnn'] = dl_train_fea1.noun_verb_adj_birnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in dl_train_fea2.columns:\n",
    "    nodl_train_fea[c] = dl_train_fea2[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(nodl_train_fea.drop([u'h_exactly_same',                                                         \n",
    "                                                                           'x_difflib_sim',\n",
    "                                                                           'DiceDistance_2gram',\n",
    " 'magic_no_stops_q1',\n",
    " 'magic_no_stops_q2',\n",
    " 'magic_stems_no_stops_q1',\n",
    " 'magic_stems_no_stops_q2',\n",
    " 'magic_stems_q1',\n",
    " 'magic_stems_q2',                                                                                                                        \n",
    "                                                                            'IntersectRatio_3gramno_stops',\n",
    "                                                                           u'CooccurrenceCount_2gramno_stops',\n",
    "                                                                           u'CooccurrenceRatio_3gramno_stops',\n",
    "                                                                           u'IntersectCount_2gramno_stops',\n",
    "                                                                           u'DiceDistance_2gramno_stops',\n",
    "                                                                           u'CooccurrenceCount_3gramno_stops',\n",
    "                                                                           u'IntersectCount_3gramno_stops',\n",
    "                                                                           u'IntersectCount_3gramno_stops',\n",
    "                                                                           'DiceDistance_3gramno_stops',\n",
    "                                                                           u'DiceDistance_3gram'],axis=1), train_df['is_duplicate'], test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_eval = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_bin':255,\n",
    "    'verbose':10,\n",
    "'min_data_in_leaf':300}\n",
    "\n",
    "# feature_name = [u'word_match', u'tfidf_word_match', u'dl_feature']\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000000,\n",
    "                valid_sets=[lgb_train,lgb_eval],\n",
    "                early_stopping_rounds=50,verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_nodl_fea = pd.read_csv('./data/test_no_dl_fea.csv')\n",
    "test_nodl_fea1 = pd.read_csv('./data/test_no_dl_fea1.csv')\n",
    "test_dl_fea = pd.read_csv('./data/dl_test.csv')\n",
    "test_dl_fea1 = pd.read_csv('./data/dl_test_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = [c for c in test_nodl_fea1.columns if ((c[:2]=='h_')or(c[:2]=='k_'))]+['w_porter_interaction',\n",
    " 'w_porter_jaccard',\n",
    " 'w_porter_levenshtein_1',\n",
    " 'w_porter_levenshtein_2',\n",
    " 'w_porter_sorensen']#\n",
    "for c in col:\n",
    "    test_nodl_fea[c] = test_nodl_fea1[c]\n",
    "for c in [u'no_stops_dl', u'clean_dl', u'no_stops_birnn_dl',\n",
    "       u'clean_dl_birnn', u'no_stops_birnn_dl_difference',\n",
    "       u'clean_birnn_dl_difference', u'stemd_birnn',\n",
    "       u'no_stops_birnn_dl_glove', u'clean_difference_dl_glove',\n",
    "       u'clean_dl_birnn_glove', u'adj_n_v_nostopsdl_glove',\n",
    "       u'stemd_birnn_glove']:#\n",
    "    test_nodl_fea[c] = test_dl_fea[c]\n",
    "test_nodl_fea['noun_verb_adj_birnn'] = test_dl_fea1.noun_verb_adj_birnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del test_nodl_fea1,test_dl_fea,test_dl_fea1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fea = test_nodl_fea.drop([u'h_exactly_same',                                                         \n",
    "                                                                           'x_difflib_sim',\n",
    "                                                                           'DiceDistance_2gram',\n",
    " 'magic_no_stops_q1',\n",
    " 'magic_no_stops_q2',\n",
    " 'magic_stems_no_stops_q1',\n",
    " 'magic_stems_no_stops_q2',\n",
    " 'magic_stems_q1',\n",
    " 'magic_stems_q2',                                                                                                                        \n",
    "                                                                            'IntersectRatio_3gramno_stops',\n",
    "                                                                           u'CooccurrenceCount_2gramno_stops',\n",
    "                                                                           u'CooccurrenceRatio_3gramno_stops',\n",
    "                                                                           u'IntersectCount_2gramno_stops',\n",
    "                                                                           u'DiceDistance_2gramno_stops',\n",
    "                                                                           u'CooccurrenceCount_3gramno_stops',\n",
    "                                                                           u'IntersectCount_3gramno_stops',\n",
    "                                                                           u'IntersectCount_3gramno_stops',\n",
    "                                                                           'DiceDistance_3gramno_stops',\n",
    "                                                                           u'DiceDistance_3gram'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del test_nodl_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbm.save_model('./ensem_model/model_6.txt')\n",
    "print('Start predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(test_fea, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_eval = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 50,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_bin':400}\n",
    "\n",
    "# feature_name = [u'word_match', u'tfidf_word_match', u'dl_feature']\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=[lgb_eval],\n",
    "                early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= nodl_train_fea.drop([u'h_exactly_same',\n",
    "                        'x_difflib_sim',\n",
    "                        'DiceDistance_2gram',\n",
    " 'magic_no_stops_q1',\n",
    " 'magic_no_stops_q2',\n",
    " 'magic_stems_no_stops_q1',\n",
    " 'magic_stems_no_stops_q2',\n",
    " 'magic_stems_q1',\n",
    " 'magic_stems_q2',                                                                                                                        \n",
    "                                                                            'IntersectRatio_3gramno_stops',\n",
    "                                                                           u'CooccurrenceCount_2gramno_stops',\n",
    "                                                                           u'CooccurrenceRatio_3gramno_stops',\n",
    "                                                                           u'IntersectCount_2gramno_stops',\n",
    "                                                                           u'DiceDistance_2gramno_stops',\n",
    "                                                                           u'CooccurrenceCount_3gramno_stops',\n",
    "                                                                           u'IntersectCount_3gramno_stops',\n",
    "                                                                           u'IntersectCount_3gramno_stops',\n",
    "                                                                           'DiceDistance_3gramno_stops',\n",
    "                                                                           u'DiceDistance_3gram'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix,hstack\n",
    "def fromsparsetofile(filename, array, deli1=\" \", deli2=\":\",ytarget=None):    \n",
    "    zsparse=csr_matrix(array)\n",
    "    indptr = zsparse.indptr\n",
    "    indices = zsparse.indices\n",
    "    data = zsparse.data\n",
    "    print(\" data lenth %d\" % (len(data)))\n",
    "    print(\" indices lenth %d\" % (len(indices)))    \n",
    "    print(\" indptr lenth %d\" % (len(indptr)))\n",
    "    \n",
    "    f=open(filename,\"w\")\n",
    "    counter_row=0\n",
    "    for b in range(0,len(indptr)-1):\n",
    "        #if there is a target, print it else , print nothing\n",
    "        if ytarget!=None:\n",
    "             f.write(str(ytarget[b]) + deli1)     \n",
    "             \n",
    "        for k in range(indptr[b],indptr[b+1]):\n",
    "            if (k==indptr[b]):\n",
    "                if np.isnan(data[k]):\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,-1))\n",
    "                else :\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,data[k]))                    \n",
    "            else :\n",
    "                if np.isnan(data[k]):\n",
    "                     f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,-1))  \n",
    "                else :\n",
    "                    f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,data[k]))\n",
    "        f.write(\"\\n\")\n",
    "        counter_row+=1\n",
    "        if counter_row%10000==0:    \n",
    "            print(\" row : %d \" % (counter_row))    \n",
    "    f.close()  \n",
    "X=csr_matrix(X)\n",
    "X =hstack([X]).tocsr()\n",
    "fromsparsetofile(\"./data/train.sparse\", X, deli1=\" \", deli2=\":\",ytarget=train_df.is_duplicate.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_nodl_fea = pd.read_csv('./data/test_no_dl_fea.csv')\n",
    "test_nodl_fea1 = pd.read_csv('./data/test_no_dl_fea1.csv')\n",
    "test_dl_fea = pd.read_csv('./data/dl_test.csv')\n",
    "test_dl_fea1 = pd.read_csv('./data/dl_test_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = [c for c in test_nodl_fea1.columns if ((c[:2]=='h_')or(c[:2]=='k_'))]+['w_porter_interaction',\n",
    " 'w_porter_jaccard',\n",
    " 'w_porter_levenshtein_1',\n",
    " 'w_porter_levenshtein_2',\n",
    " 'w_porter_sorensen']#\n",
    "for c in col:\n",
    "    test_nodl_fea[c] = test_nodl_fea1[c]\n",
    "for c in [u'no_stops_dl', u'clean_dl',u'no_stops_birnn_dl','clean_dl_birnn','no_stops_birnn_dl_difference',\n",
    "   u'clean_birnn_dl_difference','stemd_birnn','no_stops_birnn_dl_glove']:#\n",
    "    test_nodl_fea[c] = test_dl_fea[c]\n",
    "test_nodl_fea['noun_verb_adj_birnn'] = test_dl_fea1.noun_verb_adj_birnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del test_nodl_fea1,test_dl_fea,test_dl_fea1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= test_nodl_fea.drop([u'h_exactly_same',\n",
    "                        'x_difflib_sim',\n",
    "                        'DiceDistance_2gram',\n",
    " 'magic_no_stops_q1',\n",
    " 'magic_no_stops_q2',\n",
    " 'magic_stems_no_stops_q1',\n",
    " 'magic_stems_no_stops_q2',\n",
    " 'magic_stems_q1',\n",
    " 'magic_stems_q2',                                                                                                                        \n",
    "                                                                            'IntersectRatio_3gramno_stops',\n",
    "                                                                           u'CooccurrenceCount_2gramno_stops',\n",
    "                                                                           u'CooccurrenceRatio_3gramno_stops',\n",
    "                                                                           u'IntersectCount_2gramno_stops',\n",
    "                                                                           u'DiceDistance_2gramno_stops',\n",
    "                                                                           u'CooccurrenceCount_3gramno_stops',\n",
    "                                                                           u'IntersectCount_3gramno_stops',\n",
    "                                                                           u'IntersectCount_3gramno_stops',\n",
    "                                                                           'DiceDistance_3gramno_stops',\n",
    "                                                                           u'DiceDistance_3gram'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del test_nodl_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix,hstack\n",
    "def fromsparsetofile(filename, array, deli1=\" \", deli2=\":\",ytarget=None):    \n",
    "    zsparse=csr_matrix(array)\n",
    "    indptr = zsparse.indptr\n",
    "    indices = zsparse.indices\n",
    "    data = zsparse.data\n",
    "    print(\" data lenth %d\" % (len(data)))\n",
    "    print(\" indices lenth %d\" % (len(indices)))    \n",
    "    print(\" indptr lenth %d\" % (len(indptr)))\n",
    "    \n",
    "    f=open(filename,\"w\")\n",
    "    counter_row=0\n",
    "    for b in range(0,len(indptr)-1):\n",
    "        #if there is a target, print it else , print nothing\n",
    "        if ytarget!=None:\n",
    "             f.write(str(ytarget[b]) + deli1)     \n",
    "             \n",
    "        for k in range(indptr[b],indptr[b+1]):\n",
    "            if (k==indptr[b]):\n",
    "                if np.isnan(data[k]):\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,-1))\n",
    "                else :\n",
    "                    f.write(\"%d%s%f\" % (indices[k],deli2,data[k]))                    \n",
    "            else :\n",
    "                if np.isnan(data[k]):\n",
    "                     f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,-1))  \n",
    "                else :\n",
    "                    f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,data[k]))\n",
    "        f.write(\"\\n\")\n",
    "        counter_row+=1\n",
    "        if counter_row%10000==0:    \n",
    "            print(\" row : %d \" % (counter_row))    \n",
    "    f.close()  \n",
    "X=csr_matrix(X)\n",
    "X =hstack([X]).tocsr()\n",
    "fromsparsetofile(\"./data/test.sparse\", X, deli1=\" \", deli2=\":\",ytarget=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fromsparsetofile(\"./data/test.sparse\", X, deli1=\" \", deli2=\":\",ytarget=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodl_train_fea = pd.read_csv('./data/train_no_dl_fea.csv')\n",
    "#nodl_train_fea1 = pd.read_csv('./data/train_no_dl_fea1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dl_train_fea = pd.read_csv('./data/dl_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dl_train_fea.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train_org.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#log_loss(train_df.is_duplicate.values,dl_train_fea.clean_dl.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodl_train_fea.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dl_train_fea.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nodl_train_fea['is_duplicate'] = train_df.is_duplicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in nodl_train_fea1.columns:#\n",
    "    nodl_train_fea[c] = nodl_train_fea1[c]\n",
    "for c in ['clean_birnn_dl_difference',u'no_stops_dl', u'clean_dl',u'no_stops_birnn_dl','clean_dl_birnn', u'no_stops_birnn_dl_difference']:#\n",
    "    nodl_train_fea[c] = dl_train_fea[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(nodl_train_fea.drop(['IntersectCount_3gram',\n",
    "                                                                           'IntersectRatio_3gramno_stops',\n",
    "                                                                           u'CooccurrenceCount_2gramno_stops',\n",
    "                                                                           u'CooccurrenceRatio_3gramno_stops',\n",
    "                                                                           u'IntersectCount_2gramno_stops',\n",
    "                                                                           u'DiceDistance_2gramno_stops',\n",
    "                                                                           u'CooccurrenceCount_3gramno_stops',\n",
    "                                                                           u'IntersectCount_3gramno_stops',\n",
    "                                                                           u'IntersectCount_3gramno_stops',\n",
    "                                                                           'DiceDistance_3gramno_stops',\n",
    "                                                                           u'DiceDistance_3gram'],axis=1), train_df['is_duplicate'], test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kk = {}\n",
    "for name,num in zip(gbm.feature_name(),list(gbm.feature_importance())):\n",
    "    kk[name] = num\n",
    "#print('Save model...')\n",
    "print('Feature importances:', )\n",
    "sorted(kk.iteritems(), key=lambda d:d[1], reverse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dl_test_fea = pd.read_csv('./data/dl_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in nodl_test_fea1.columns:\n",
    "    nodl_test_fea[c] = nodl_test_fea1[c]\n",
    "for c in ['clean_birnn_dl_difference',u'no_stops_dl', u'clean_dl',u'no_stops_birnn_dl','clean_dl_birnn', u'no_stops_birnn_dl_difference']:#\n",
    "    nodl_test_fea[c] = dl_test_fea[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(trainx,valx,trainy,valy,num_c):\n",
    "    lgb_train = lgb.Dataset(trainx, trainy)\n",
    "    lgb_eval = lgb.Dataset(valx.iloc[0:5000], valy.iloc[0:5000], reference=lgb_train)\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'num_leaves': 25,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_bin':400}\n",
    "\n",
    "    # feature_name = [u'word_match', u'tfidf_word_match', u'dl_feature']\n",
    "    print('Start training...')\n",
    "    # train\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=500,\n",
    "                    valid_sets=[lgb_eval],\n",
    "                    early_stopping_rounds=5)\n",
    "    print('Save model...')\n",
    "    gbm.save_model('./ensem_model/model_'+str(num_c)+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=8)\n",
    "train_test_split = []\n",
    "train_ids = []\n",
    "for train_index, test_index in skf.split(train_df.id, train_df.is_duplicate):\n",
    "    train_test_split.append([train_index,test_index])\n",
    "    train_ids.extend(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "train_feature = []\n",
    "test_feature = []\n",
    "for num_c,spl in enumerate(train_test_split):\n",
    "    train_x,val_x = nodl_train_fea.iloc[spl[0]],nodl_train_fea.iloc[spl[1]]\n",
    "    train_y,val_y = train_df.is_duplicate.iloc[spl[0]],train_df.is_duplicate.iloc[spl[1]]\n",
    "    best_model_epo = train_model(train_x,val_x,train_y,val_y,num_c)\n",
    "    #train_feature.extend(train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(path,val_x):\n",
    "    bst = lgb.Booster(model_file=path)\n",
    "    # can only predict with the best iteration (or the saving iteration)\n",
    "    y_pred = bst.predict(val_x)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feature = []\n",
    "test_feature = []\n",
    "for num_c,spl in enumerate(train_test_split):\n",
    "    path = './ensem_model/model_'+str(num_c)+'.txt'\n",
    "    val_x = nodl_train_fea.iloc[spl[1]]\n",
    "    train_pred = test_model(path,val_x)\n",
    "    train_feature.extend(train_pred)\n",
    "    #test_feature.append(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dl = np.zeros(len(train_ids))\n",
    "for i,id_ in enumerate(train_ids):\n",
    "    train_dl[id_] = train_feature[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_eval = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 25,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_bin':400}\n",
    "\n",
    "# feature_name = [u'word_match', u'tfidf_word_match', u'dl_feature']\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm1 = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=[lgb_eval],\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "print('Save model...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbm.save_model('./ensem_model/model_2.txt')\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(test_nodl_fea, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_o = pd.read_csv('test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = 0.165 / 0.37    \n",
    "# b = (1 - 0.165) / (1 - 0.37)\n",
    "# final_pred = map(lambda x: (a*x/(a*x + b*(1-x))), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 0.165 / 0.37    \n",
    "b = (1 - 0.165) / (1 - 0.37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_dl = np.zeros_like(test_dl_featuers[\"model_0\"].values)\n",
    "for i in range(len(ensem_files)):\n",
    "    x_test_dl = x_test_dl + test_dl_featuers[\"model_\"+str(i)].apply(lambda x: (a*x/(a*x + b*(1-x))))*weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_eval = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 25,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_bin':400}\n",
    "\n",
    "# feature_name = [u'word_match', u'tfidf_word_match', u'dl_feature']\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=[lgb_eval],\n",
    "                early_stopping_rounds=15)\n",
    "\n",
    "print('Save model...')\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "# print('Start predicting...')\n",
    "# # predict\n",
    "# y_pred = gbm.predict(x_test, num_iteration=gbm.best_iteration)\n",
    "# # eval\n",
    "# print('The rmse of prediction is:', log_loss(y_test, y_pred))\n",
    "\n",
    "# print('Dump model to JSON...')\n",
    "# # dump model to json (and save to file)\n",
    "# model_json = gbm.dump_model()\n",
    "\n",
    "# with open('model.json', 'w+') as f:\n",
    "#     json.dump(model_json, f, indent=4)\n",
    "\n",
    "\n",
    "print('Feature names:', gbm.feature_name())\n",
    "\n",
    "print('Calculate feature importances...')\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importance()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_train = x_train[y_train == 1]\n",
    "neg_train = x_train[y_train == 0]\n",
    "\n",
    "# Now we oversample the negative class\n",
    "# There is likely a much more elegant way to do this...\n",
    "p = 0.165\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "x_train = pd.concat([pos_train, neg_train])\n",
    "y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "del pos_train, neg_train\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
