{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# checkpoint_dir = '~/research/quora/quora_train/1491988932/checkpoints/'\n",
    "# checkpoint_file= tf.train.latest_checkpoint(checkpoint_dir)\n",
    "# print(\"checkpoint file: {}\".format(checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test1 = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "output = open('vacabulary_quora_adab.pkl', 'r')\n",
    "vocabulary = pickle.load(output)\n",
    "output.close()\n",
    "output = open('vocabulary_inv_quora_adab.pkl', 'r') \n",
    "vocabulary_inv = pickle.load(output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def nltk_tokenize(s):    \n",
    "    s = s.lower().strip()    \n",
    "    return nltk.word_tokenize(s)\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.,!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s.split(' ')\n",
    "\n",
    "def get_pairs(df_text,test_with_label= False):\n",
    "    pairs_test = []\n",
    "    if test_with_label:\n",
    "        for q1,q2,is_dup in zip(df_text.question1,df_text.question2,df_text.is_duplicate):\n",
    "            if type(q1) ==float:\n",
    "                q1 = \"<mino/>\"\n",
    "            try:\n",
    "                pair1 =  nltk_tokenize(q1)\n",
    "            except:\n",
    "                pair1 =  normalizeString(q1)\n",
    "            if type(q2) ==float:\n",
    "                q2 = \"<mino/>\"\n",
    "            try:\n",
    "                pair2 =  nltk_tokenize(q2)\n",
    "            except:\n",
    "                pair2 =  normalizeString(q2)\n",
    "            pairs_test.append([pair1,pair2,is_dup])\n",
    "    else:\n",
    "        for q1,q2 in zip(df_text.question1,df_text.question2):\n",
    "            if type(q1) ==float:\n",
    "                q1 = \"<mino/>\"\n",
    "            try:\n",
    "                pair1 =  nltk_tokenize(q1)\n",
    "            except:\n",
    "                pair1 =  normalizeString(q1)\n",
    "            if type(q2) ==float:\n",
    "                q2 = \"<mino/>\"\n",
    "            try:\n",
    "                pair2 =  nltk_tokenize(q2)\n",
    "            except:\n",
    "                pair2 =  normalizeString(q2)\n",
    "            pairs_test.append([pair1,pair2])\n",
    "    return pairs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs_test1 = get_pairs(df_test1,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(pairs_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data(input_data, vocabulary,training = True):\n",
    "    mino_idx = vocabulary[\"<mino/>\"]\n",
    "    data_index = []\n",
    "    for pair in input_data:\n",
    "        sentence1 = [vocabulary.get(word, mino_idx) for word in pair[0]]\n",
    "        sentence2 = [vocabulary.get(word, mino_idx) for word in pair[1]]\n",
    "        if training:\n",
    "            data_index.append([sentence1,sentence2,pair[2]])\n",
    "        else:\n",
    "            data_index.append([sentence1,sentence2])\n",
    "    return data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(batch,training = True):\n",
    "    length1 = [len(sen1) for sen1 in batch[:,0]]\n",
    "    max_length1 = max(length1)\n",
    "    length2 = [len(sen2) for sen2 in batch[:,1]]\n",
    "    max_length2 = max(length2)\n",
    "    s1_batch = []\n",
    "    s2_batch = []\n",
    "    for sen in batch:\n",
    "        if len(sen[0])==max_length1:\n",
    "            s1_batch.append(sen[0])\n",
    "        else:\n",
    "            s1_batch.append(sen[0]+(max_length1-len(sen[0]))*[0])\n",
    "        if len(sen[1])==max_length2:\n",
    "            s2_batch.append(sen[1])\n",
    "        else:\n",
    "            s2_batch.append(sen[1]+(max_length2-len(sen[1]))*[0])\n",
    "    if training:   \n",
    "        return np.array(s1_batch),np.array(s2_batch),batch[:,2],length1,length2\n",
    "    else:\n",
    "        return np.array(s1_batch),np.array(s2_batch),length1,length2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_t_size = 512\n",
    "test_index = build_data(pairs_test1,vocabulary,training =True)\n",
    "pad_num = (len(test_index)/batch_t_size+1)*batch_t_size-len(test_index)\n",
    "for i in range(pad_num):\n",
    "    test_index.append([[0,0],[0,0],1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = '~/research/quora/quora_train/1493868214/checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault('CUDA_VISIBLE_DEVICES','0')\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n",
    "best_loss = 10\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        ckpt_list = ckpt.all_model_checkpoint_paths\n",
    "        for file_num in range(len(ckpt_list)):\n",
    "            file_name = ckpt_list[file_num]\n",
    "            saver = tf.train.import_meta_graph(\"{}.meta\".format(file_name))\n",
    "            saver.restore(sess, file_name)\n",
    "            # Get the placeholders from the graph by name\n",
    "            input_t_s1 = graph.get_operation_by_name(\"input_placeholder_x1\").outputs[0]\n",
    "            input_t_s2 = graph.get_operation_by_name(\"input_placeholder_x2\").outputs[0]\n",
    "            lengtht_1 = graph.get_operation_by_name(\"length1\").outputs[0]\n",
    "            lengtht_2 = graph.get_operation_by_name(\"length2\").outputs[0]\n",
    "            diff_length = graph.get_operation_by_name(\"diff_length\").outputs[0]\n",
    "            predictions = graph.get_operation_by_name(\"m3_prediction/m3_out\").outputs[0]\n",
    "            pred_d =[]\n",
    "            true_labels = []\n",
    "            for j in range(len(test_index)/batch_t_size):\n",
    "                batch_s1_d,batch_s2_d,batch_labels,length1_d, length2_d= get_batch(np.array(test_index[j*batch_t_size:(j+1)*batch_t_size]),training =True)\n",
    "                diff_length_Bat = np.abs(np.subtract(length1_d,length2_d))\n",
    "                diff_length_Bat[diff_length_Bat>79]=79\n",
    "                feed_dict = {\n",
    "                    input_t_s1: batch_s1_d,\n",
    "                    input_t_s2: batch_s2_d,\n",
    "                    lengtht_1:length1_d,\n",
    "                    lengtht_2:length2_d,\n",
    "                    diff_length: diff_length_Bat\n",
    "                }\n",
    "                pd = sess.run(predictions,feed_dict)\n",
    "                true_labels.extend(batch_labels)\n",
    "                pred_d.extend(pd[:,1].astype(float))\n",
    "                if j%400 ==0:\n",
    "                    print('step: '+str(j))\n",
    "            current_loss = log_loss(true_labels[:-pad_num], pred_d[:-pad_num])\n",
    "            print('loss: '+str(current_loss)+'file: '+ file_name)\n",
    "            if current_loss < best_loss:\n",
    "                best_loss= current_loss\n",
    "                bestfile = file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('models_quora.pkl', 'r') as f:\n",
    "    ensemfiles = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault('CUDA_VISIBLE_DEVICES','0')\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n",
    "best_loss = 10\n",
    "nums_correct = []\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        for file_name in ensemfiles[1:]:\n",
    "            #file_name = ckpt_list[file_num]\n",
    "            saver = tf.train.import_meta_graph(\"{}.meta\".format(file_name))\n",
    "            saver.restore(sess, file_name)\n",
    "            # Get the placeholders from the graph by name\n",
    "            input_t_s1 = graph.get_operation_by_name(\"input_placeholder_x1\").outputs[0]\n",
    "            input_t_s2 = graph.get_operation_by_name(\"input_placeholder_x2\").outputs[0]\n",
    "            lengtht_1 = graph.get_operation_by_name(\"length1\").outputs[0]\n",
    "            lengtht_2 = graph.get_operation_by_name(\"length2\").outputs[0]\n",
    "            diff_length = graph.get_operation_by_name(\"diff_length\").outputs[0]\n",
    "            predictions = graph.get_operation_by_name(\"m3_prediction/m3_out\").outputs[0]\n",
    "            pred_d =[]\n",
    "            true_labels = []\n",
    "            predprob = []\n",
    "            for j in range(len(test_index)/batch_t_size):\n",
    "                batch_s1_d,batch_s2_d,batch_labels,length1_d, length2_d= get_batch(np.array(test_index[j*batch_t_size:(j+1)*batch_t_size]),training =True)\n",
    "                diff_length_Bat = np.abs(np.subtract(length1_d,length2_d))\n",
    "                diff_length_Bat[diff_length_Bat>79]=79\n",
    "                feed_dict = {\n",
    "                    input_t_s1: batch_s1_d,\n",
    "                    input_t_s2: batch_s2_d,\n",
    "                    lengtht_1:length1_d,\n",
    "                    lengtht_2:length2_d,\n",
    "                    diff_length: diff_length_Bat\n",
    "                }\n",
    "                pd = sess.run(predictions,feed_dict)\n",
    "                true_labels.extend(batch_labels)\n",
    "                pred_d.extend(pd[:,1].astype(float))\n",
    "                predprob.extend(pd)\n",
    "            nums_correct.append(np.sum(np.equal(true_labels[:-pad_num],np.argmax(predprob[:-pad_num],axis=1)).astype(float)))   \n",
    "            current_loss = log_loss(true_labels[:-pad_num], pred_d[:-pad_num])\n",
    "            print('loss: '+str(current_loss)+'file: '+ file_name)\n",
    "            if current_loss < best_loss:\n",
    "                best_loss= current_loss\n",
    "                bestfile = file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas = nums_correct/sum(nums_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_pro_all = np.zeros(len(test_index))\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        for num_f,file_name in enumerate(ensemfiles[1:]):\n",
    "            saver = tf.train.import_meta_graph(\"{}.meta\".format(file_name))\n",
    "            saver.restore(sess, file_name)\n",
    "            # Get the placeholders from the graph by name\n",
    "            input_t_s1 = graph.get_operation_by_name(\"input_placeholder_x1\").outputs[0]\n",
    "            input_t_s2 = graph.get_operation_by_name(\"input_placeholder_x2\").outputs[0]\n",
    "            lengtht_1 = graph.get_operation_by_name(\"length1\").outputs[0]\n",
    "            lengtht_2 = graph.get_operation_by_name(\"length2\").outputs[0]\n",
    "            diff_length = graph.get_operation_by_name(\"diff_length\").outputs[0]\n",
    "            predictions = graph.get_operation_by_name(\"m3_prediction/m3_out\").outputs[0]\n",
    "            pred_d =[]\n",
    "            for j in range(len(test_index)/batch_t_size):\n",
    "                batch_s1_d,batch_s2_d,length1_d,length2_d= get_batch(np.array(test_index[j*batch_t_size:(j+1)*batch_t_size]),training =False)\n",
    "                diff_length_Bat = np.abs(np.subtract(length1_d,length2_d))\n",
    "                diff_length_Bat[diff_length_Bat>79]=79\n",
    "                feed_dict = {\n",
    "                    input_t_s1: batch_s1_d,\n",
    "                    input_t_s2: batch_s2_d,\n",
    "                    lengtht_1:length1_d,\n",
    "                    lengtht_2:length2_d,\n",
    "                    diff_length: diff_length_Bat\n",
    "                }\n",
    "                pd = sess.run(predictions,feed_dict)\n",
    "                pred_d.extend(pd[:,1])\n",
    "                if j%400 ==0:\n",
    "                    print('file: '+ str(num_f) + '  step: '+str(j))\n",
    "            pred_pro_all = np.dot(pred_d,alphas[num_f])+pred_pro_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_loss(true_labels[:4000], pred_pro_all[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(pred_pro_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test_final = pd.read_csv('test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_t_size = 512\n",
    "pairs_test_final = get_pairs(df_test_final,False)\n",
    "test_final_index = build_data(pairs_test_final,vocabulary,training =False)\n",
    "pad_num = (len(test_final_index)/batch_t_size+1)*batch_t_size-len(test_final_index)\n",
    "for i in range(pad_num):\n",
    "    test_final_index.append([[0,0],[0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bestfile = '/home/zhangjinbin/research/quora/quora_train/1492012667/checkpoints/model-11844'\n",
    "pred_pro_all = np.zeros(len(test_final_index))\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        for num_f,file_name in enumerate(ensemfiles[1:]):\n",
    "            saver = tf.train.import_meta_graph(\"{}.meta\".format(file_name))\n",
    "            saver.restore(sess, file_name)\n",
    "            # Get the placeholders from the graph by name\n",
    "            input_t_s1 = graph.get_operation_by_name(\"input_placeholder_x1\").outputs[0]\n",
    "            input_t_s2 = graph.get_operation_by_name(\"input_placeholder_x2\").outputs[0]\n",
    "            lengtht_1 = graph.get_operation_by_name(\"length1\").outputs[0]\n",
    "            lengtht_2 = graph.get_operation_by_name(\"length2\").outputs[0]\n",
    "            diff_length = graph.get_operation_by_name(\"diff_length\").outputs[0]\n",
    "            predictions = graph.get_operation_by_name(\"m3_prediction/m3_out\").outputs[0]\n",
    "            pred_d =[]\n",
    "            for j in range(len(test_final_index)/batch_t_size):\n",
    "                batch_s1_d,batch_s2_d,length1_d,length2_d= get_batch(np.array(test_final_index[j*batch_t_size:(j+1)*batch_t_size]),training =False)\n",
    "                diff_length_Bat = np.abs(np.subtract(length1_d,length2_d))\n",
    "                diff_length_Bat[diff_length_Bat>79]=79\n",
    "                feed_dict = {\n",
    "                    input_t_s1: batch_s1_d,\n",
    "                    input_t_s2: batch_s2_d,\n",
    "                    lengtht_1:length1_d,\n",
    "                    lengtht_2:length2_d,\n",
    "                    diff_length: diff_length_Bat\n",
    "                }\n",
    "                pd = sess.run(predictions,feed_dict)\n",
    "                pred_d.extend(pd[:,1].astype(float))\n",
    "                if j%400 ==0:\n",
    "                    print('file: '+ str(num_f) + '  step: '+str(j))\n",
    "            pred_d = map(lambda x: (a*x/(a*x + b*(1-x))),pred_d)\n",
    "            pred_pro_all = np.dot(pred_d,alphas[num_f])+pred_pro_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
