{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.optimize import minimize\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "import xgboost as xgb\n",
    "import multiprocessing\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train_org.csv\")\n",
    "df_test = pd.read_csv(\"test_final.csv\")\n",
    "df_train_clean = pd.read_csv('./processed_data/train_clean.csv')\n",
    "df_test_clean = pd.read_csv('./processed_data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q1= \"question1\"\n",
    "q2= \"question2\"\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "tfidf_txt = pd.Series(df_train[q1].tolist() + df_train[q2].tolist() +\n",
    "                      df_test[q1].tolist() + df_test[q2].tolist()).astype(str)\n",
    "tfidf.fit_transform(tfidf_txt)\n",
    "def diff_ratios(st1, st2):\n",
    "    seq = difflib.SequenceMatcher()\n",
    "    seq.set_seqs(str(st1).lower(), str(st2).lower())\n",
    "    return seq.ratio()\n",
    "\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (float(len(shared_words_in_q1)) + float(len(shared_words_in_q2)))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def get_features_1(df_features):\n",
    "    print('nouns...')\n",
    "    df_features['question1_nouns'] = df_features.question1.map(lambda x: [w for w, t in nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8').strip().lower() if type(x)!=float else str(x))) if t[:1] in ['N']])\n",
    "    df_features['question2_nouns'] = df_features.question2.map(lambda x: [w for w, t in nltk.pos_tag(nltk.word_tokenize(x.decode('utf-8').strip().lower() if type(x)!=float else str(x))) if t[:1] in ['N']])\n",
    "    df_features['z_noun_match'] = df_features.apply(lambda r: sum([1 for w in r.question1_nouns if w in r.question2_nouns]), axis=1)  #takes long\n",
    "    print('lengths...')\n",
    "    df_features['z_len1'] = df_features.question1.map(lambda x: len(str(x)))\n",
    "    df_features['z_len2'] = df_features.question2.map(lambda x: len(str(x)))\n",
    "    df_features['z_word_len1'] = df_features.question1.map(lambda x: len(str(x).split()))\n",
    "    df_features['z_word_len2'] = df_features.question2.map(lambda x: len(str(x).split()))\n",
    "    print('difflib...')\n",
    "    df_features['z_match_ratio'] = df_features.apply(lambda r: diff_ratios(r.question1, r.question2), axis=1)  #takes long\n",
    "    print('word match...')\n",
    "    df_features['z_word_match'] = df_features.apply(word_match_share, axis=1, raw=True)\n",
    "    print('tfidf...')\n",
    "    df_features['z_tfidf_sum1'] = df_features.question1.map(lambda x: np.sum(tfidf.transform([str(x)]).data))\n",
    "    df_features['z_tfidf_sum2'] = df_features.question2.map(lambda x: np.sum(tfidf.transform([str(x)]).data))\n",
    "    df_features['z_tfidf_mean1'] = df_features.question1.map(lambda x: np.mean(tfidf.transform([str(x)]).data))\n",
    "    df_features['z_tfidf_mean2'] = df_features.question2.map(lambda x: np.mean(tfidf.transform([str(x)]).data))\n",
    "    df_features['z_tfidf_len1'] = df_features.question1.map(lambda x: len(tfidf.transform([str(x)]).data))\n",
    "    df_features['z_tfidf_len2'] = df_features.question2.map(lambda x: len(tfidf.transform([str(x)]).data))\n",
    "    df_features = df_features.fillna(0.0)\n",
    "    col = [c for c in df_features.columns if c[:1]=='z']\n",
    "    return df_features[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_clean = pd.read_csv('./processed_data/train_clean.csv')\n",
    "df_test_clean = pd.read_csv('./processed_data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "train_qs = pd.Series(df_train_clean['question1'].tolist() + df_train_clean['question2'].tolist()).astype(str)\n",
    "test_qs = pd.Series(df_test_clean['question1'].tolist() + df_test_clean['question2'].tolist()).astype(str)\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1.0 / (count + eps)\n",
    "eps = 5000 \n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1_list']:#).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    \n",
    "    for word in row['question2_list']:#).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights).astype(float) / np.sum(total_weights)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "import difflib\n",
    "def extract_fea_2(df_):\n",
    "    data = pd.DataFrame()\n",
    "    #distance feature\n",
    "    data['len_q1'] = df_['question1'].apply(lambda x: len(str(x)))\n",
    "    data['len_q2'] = df_['question2'].apply(lambda x: len(str(x)))\n",
    "    data['x_diff_len'] = abs(data['len_q1'] - data['len_q2'])\n",
    "    data['x_len_char_q1'] = df_['question1'].apply(lambda x: len(set(str(x))))\n",
    "    data['x_len_char_q2'] = df_['question2'].apply(lambda x: len(set(str(x))))\n",
    "    data['x_diff_char_len'] = abs(data['x_len_char_q1'] - data['x_len_char_q2'])\n",
    "    data['len_word_q1'] = df_['question1'].apply(lambda x:len(str(x).split()))\n",
    "    data['len_word_q2'] = df_['question2'].apply(lambda x:len(str(x).split()))\n",
    "    data['x_diff_word_len'] = abs(data['len_word_q1'] - data['len_word_q2'])\n",
    "    \n",
    "    #jaccard_similarity\n",
    "    def jaccard_similarity(a, b):\n",
    "        a, b = set(a), set(b)\n",
    "        c = a & b\n",
    "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "    data['x_common_unigram_len'] = df_.apply(lambda x: len(set(str(x['question1']).split()).intersection(set(str(x['question2']).split()))), axis=1)\n",
    "    data['x_match_char_jaccard'] = df_.apply(lambda x: jaccard_similarity(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['x_match_word_jaccard'] = df_.apply(lambda x: jaccard_similarity(str(x['question1']).split(),str(x['question2']).split()), axis=1)\n",
    "    #edit distance\n",
    "    data['x_levenshtein_ratio'] = df_.apply(lambda x: Levenshtein.ratio(str(x['question1']),str(x['question2'])), axis=1)\n",
    "    data['x_levenshtein_seqratio'] = df_.apply(lambda x: Levenshtein.seqratio(str(x['question1']).split(),str(x['question2']).split()),axis=1)\n",
    "    data['x_levenshtein_setratio'] = df_.apply(lambda x: Levenshtein.setratio(str(x['question1']).split(),str(x['question2']).split()), axis=1)\n",
    "    #word matching\n",
    "    data['x_difflib_sim'] = df_.apply(lambda x: difflib.SequenceMatcher(None, str(x['question1']), str(x['question2'])).ratio(), axis=1)\n",
    "    data['x_word_match_tf_idf'] = df_.apply(word_match_share, axis=1)\n",
    "    col = [c for c in data.columns if c[:1]=='x']\n",
    "    return data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "def gen_fuzz_feature_3(df_):\n",
    "    data = pd.DataFrame()\n",
    "    data['fuzz_qratio'] = df_.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_WRatio'] = df_.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_partial_ratio'] = df_.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_token_sort_ratio'] = df_.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_partial_token_sort_ratio'] = df_.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#magic feature\n",
    "def magic_feature(q1,q2,train,test):\n",
    "    train_orig = train.copy()\n",
    "    test_orig = test.copy()\n",
    "    train_question1 = train_orig[[q1]].copy()\n",
    "    train_question2 = train_orig[[q2]].copy()\n",
    "    test_question1 = test_orig[[q1]].copy()\n",
    "    test_question2 = test_orig[[q2]].copy()\n",
    "\n",
    "    train_question2.rename(columns={q2: q1}, inplace=True)\n",
    "    test_question2.rename(columns={q2: q1}, inplace=True)\n",
    "\n",
    "    questions = train_question1.append(train_question2)\n",
    "    questions = questions.append(test_question1)\n",
    "    questions = questions.append(test_question2)\n",
    "    questions.drop_duplicates(subset=[q1], inplace=True)\n",
    "    questions.reset_index(inplace=True, drop=True)\n",
    "    questions_dict = pd.Series(questions.index.values, index=questions[q1].values).to_dict()\n",
    "\n",
    "    train_cp = train_orig.copy()\n",
    "    test_cp = test_orig.copy()\n",
    "    test_cp['is_duplicate'] = -1\n",
    "    test_cp.rename(columns={'test_id': 'id'}, inplace=True)\n",
    "    comb = pd.concat([train_cp, test_cp])\n",
    "    comb['q1_hash'] = comb[q1].map(questions_dict)\n",
    "    comb['q2_hash'] = comb[q2].map(questions_dict)\n",
    "\n",
    "    q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "    q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "    def try_apply_dict(x, dict_to_apply):\n",
    "        try:\n",
    "            return dict_to_apply[x]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "    comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x, q1_vc) + try_apply_dict(x, q2_vc))\n",
    "    comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x, q1_vc) + try_apply_dict(x, q2_vc))\n",
    "\n",
    "    train_features = comb[comb['is_duplicate'] >= 0][['q1_freq', 'q2_freq']]\n",
    "    test_features = comb[comb['is_duplicate'] < 0][['q1_freq', 'q2_freq']]\n",
    "    del train_orig\n",
    "    del test_orig\n",
    "    return train_features,test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodl_train_fea = pd.read_csv('./processed_data/train_extract_nodl_fea.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodl_train_fea.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodl_test_fea = pd.read_csv('./processed_data/test_extract_nodl_fea.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_extract_1 = get_features_1(df_train_clean)\n",
    "df_test_extract_1 = get_features_1(df_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_extract_2 = extract_fea_2(df_train_clean)\n",
    "df_test_extract_2 = extract_fea_2(df_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_feature(df_pre,df_now):\n",
    "    for c in df_now.columns:\n",
    "        df_pre[c] = df_now[c]\n",
    "    return df_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_train_feature2 = get_new_feature(df_train_extract_1,df_train_extract_2)\n",
    "extract_test_feature2 = get_new_feature(df_test_extract_1,df_test_extract_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fuzz_3 = gen_fuzz_feature_3(df_train_clean)\n",
    "test_fuzz_3 = gen_fuzz_feature_3(df_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_train_feature3 = get_new_feature(extract_train_feature2,train_fuzz_3)\n",
    "extract_test_feature3 = get_new_feature(extract_test_feature2,test_fuzz_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract_train_feature3.to_csv('./processed_data/train_no_dl_fea.csv',index=False)\n",
    "# extract_test_feature3.to_csv('./processed_data/test_no_dl_fea.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def magic_feature_all_4(df_train_clean,df_test_clean):\n",
    "    data_train = pd.DataFrame()\n",
    "    data_test = pd.DataFrame()\n",
    "    flags = ['clean','no_stops','stems','stems_no_stops']\n",
    "    for flag in flags:\n",
    "        train_magic,test_magic = magic_feature('q1_'+flag,'q2_'+flag,df_train_clean,df_test_clean)\n",
    "        data_train['magic_'+flag+'_q1'] = train_magic.q1_freq\n",
    "        data_train['magic_'+flag+'_q2'] = train_magic.q2_freq\n",
    "\n",
    "        data_test['magic_'+flag+'_q1'] = test_magic.q1_freq\n",
    "        data_test['magic_'+flag+'_q2']= test_magic.q2_freq\n",
    "    return data_train,data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "magic_train,magic_test = magic_feature_all_4(df_train_clean,df_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_train_feature4 = get_new_feature(extract_train_feature3,magic_train)\n",
    "extract_test_feature4 = get_new_feature(extract_test_feature3,magic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract_train_feature4.to_csv('./processed_data/train_no_dl_fea.csv',index=False)\n",
    "# extract_test_feature4.to_csv('./processed_data/test_no_dl_fea.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data =df_train_clean.question1.apply(lambda x: str(x).split()).values+df_train_clean.question2.apply(lambda x: str(x).split()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_list_data = np.concatenate(all_data,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _try_divide(x, y, val=0.0):\n",
    "    \"\"\"try to divide two numbers\"\"\"\n",
    "    if y != 0.0:\n",
    "        val = float(x) / y\n",
    "    return val\n",
    "def _is_str_match(str1, str2, threshold=1.0):\n",
    "    assert threshold >= 0.0 and threshold <= 1.0, \"Wrong threshold.\"\n",
    "    if float(threshold) == 1.0:\n",
    "        return str1 == str2\n",
    "    else:\n",
    "        return (1. - _edit_dist(str1, str2)) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "def jaccard_coef(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return _try_divide(float(len(A.intersection(B))), len(A.union(B)))\n",
    "def dice_dist(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return _try_divide(2.*float(len(A.intersection(B))), (len(A) + len(B)))\n",
    "def edit_dist(str1, str2):\n",
    "    try:\n",
    "        # very fast\n",
    "        # http://stackoverflow.com/questions/14260126/how-python-levenshtein-ratio-is-computed\n",
    "        # d = Levenshtein.ratio(str1, str2)\n",
    "        d = Levenshtein.distance(str1, str2)/float(max(len(str1),len(str2)))\n",
    "    except:\n",
    "        # https://docs.python.org/2/library/difflib.html\n",
    "        d = 1. - SequenceMatcher(lambda x: x==\" \", str1, str2).ratio()\n",
    "    return d\n",
    "import lzma\n",
    "def compression_dist(x, y, l_x=None, l_y=None):\n",
    "    if x == y:\n",
    "        return 0\n",
    "    x_b = x.encode('utf-8')\n",
    "    y_b = y.encode('utf-8')\n",
    "    if l_x is None:\n",
    "        l_x = len(lzma.compress(x_b))\n",
    "        l_y = len(lzma.compress(y_b))\n",
    "    l_xy = len(lzma.compress(x_b+y_b))\n",
    "    l_yx = len(lzma.compress(y_b+x_b))\n",
    "    dist = _try_divide(min(l_xy,l_yx)-min(l_x,l_y), max(l_x,l_y))\n",
    "    return dist\n",
    "def cosine_d(a,b):\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    d = len(a)*len(b)\n",
    "    if (d == 0):\n",
    "        return 0.0\n",
    "    else: \n",
    "        return float(len(a.intersection(b)))/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IntersectCount_Ngram(obs_ngrams,target_ngrams):\n",
    "    s = 0.0\n",
    "    for w1 in obs_ngrams:\n",
    "        for w2 in target_ngrams:\n",
    "            if _is_str_match(w1, w2):\n",
    "                s += 1.\n",
    "                break\n",
    "    return s\n",
    "def IntersectRatio_Ngram(obs_ngrams,target_ngrams):\n",
    "    s = 0.\n",
    "    for w1 in obs_ngrams:\n",
    "        for w2 in target_ngrams:\n",
    "            if _is_str_match(w1, w2):\n",
    "                s += 1.\n",
    "                break\n",
    "    return _try_divide(s, len(obs_ngrams))\n",
    "def CooccurrenceCount_Ngram(obs_ngrams,target_ngrams):\n",
    "    s = 0.\n",
    "    for w1 in obs_ngrams:\n",
    "        for w2 in target_ngrams:\n",
    "            if _is_str_match(w1, w2):\n",
    "                s += 1.\n",
    "    return s\n",
    "def CooccurrenceRatio_Ngram(obs_ngrams,target_ngrams):\n",
    "    s = 0.\n",
    "    for w1 in obs_ngrams:\n",
    "        for w2 in target_ngrams:\n",
    "            if _is_str_match(w1, w2):\n",
    "                s += 1.\n",
    "    return _try_divide(s, len(obs_ngrams)*len(target_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "def feature5(df_,q1,q2):\n",
    "    data = pd.DataFrame()\n",
    "    data['JaccardCoef_2gram'] = df_.apply(lambda x: jaccard_coef(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),2)),\n",
    "                                                                     list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),2))),\n",
    "                                                  axis=1)\n",
    "    data['DiceDistance_2gram'] = df_.apply(lambda x: dice_dist(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),2)),\n",
    "                                                                     list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),2))),\n",
    "                                                  axis=1)\n",
    "    data['JaccardCoef_3gram'] = df_.apply(lambda x: jaccard_coef(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),3)),\n",
    "                                                                     list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),3))),\n",
    "                                                  axis=1)\n",
    "    data['DiceDistance_3gram'] = df_.apply(lambda x: dice_dist(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),3)),\n",
    "                                                                     list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),3))),axis=1)\n",
    "#   data['CompressionDistance'] = df_.apply(lambda x: compression_dist(str(x[q1]).decode('utf-8'),str(x[q2]).decode('utf-8')),axis=1)\n",
    "    data['edit_dist'] = df_.apply(lambda x: edit_dist(str(x[q1]),str(x[q2])),axis=1)\n",
    "    data['cosine_dist'] = df_.apply(lambda x: cosine_d(str(x[q1]).split(),str(x[q2]).split()),axis=1)\n",
    "    data['IntersectCount_2gram'] = df_.apply(lambda x: IntersectCount_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),2)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),2))),\n",
    "                                                   axis=1)\n",
    "    data['IntersectRatio_2gram'] = df_.apply(lambda x: IntersectRatio_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),2)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),2))),\n",
    "                                                   axis=1)\n",
    "    data['CooccurrenceCount_2gram'] = df_.apply(lambda x: CooccurrenceCount_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),2)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),2))),\n",
    "                                                   axis=1)\n",
    "    data['CooccurrenceRatio_2gram'] = df_.apply(lambda x: CooccurrenceRatio_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),2)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),2))),\n",
    "                                                   axis=1)\n",
    "    data['IntersectCount_3gram'] = df_.apply(lambda x: IntersectCount_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),3)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),3))),\n",
    "                                                   axis=1)\n",
    "    data['IntersectRatio_3gram'] = df_.apply(lambda x: IntersectRatio_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),3)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),3))),\n",
    "                                                   axis=1)\n",
    "    data['CooccurrenceCount_3gram'] = df_.apply(lambda x: CooccurrenceCount_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),3)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),3))),\n",
    "                                                   axis=1)\n",
    "    data['CooccurrenceRatio_3gram'] = df_.apply(lambda x: CooccurrenceRatio_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),3)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),3))),\n",
    "                                                   axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_fea_train5 =feature5(df_train_clean,'question1','question2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_fea_train5.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_fea_test5 =feature5(df_test_clean,'question1','question2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_fea_test5.to_csv('./processed_data/fea_5_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_fea_train5.to_csv('./processed_data/fea_5_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodl_train_fea = pd.read_csv('./processed_data/train_no_dl_fea.csv')\n",
    "nodl_test_fea = pd.read_csv('./processed_data/test_no_dl_fea.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in data_fea_train5.columns:\n",
    "    nodl_train_fea[c] = data_fea_train5[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in data_fea_test5.columns:\n",
    "    nodl_test_fea[c] = data_fea_test5[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodl_train_fea.to_csv('./processed_data/train_no_dl_fea.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodl_train_fea.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_common_word(str1, str2, minLength=1, string_only=False):\n",
    "    word_list=[]\n",
    "    num=0\n",
    "    total_entries=0\n",
    "    cnt_letters=0\n",
    "    cnt_unique_letters=0\n",
    "    all_num=0\n",
    "    all_total_entries=0\n",
    "    all_cnt_letters=0\n",
    "    for word in str1.split():\n",
    "         if len(word)>=minLength:\n",
    "                if string_only==False or len(re.findall(r'\\d+', word))==0:\n",
    "                    if (' '+word+' ') in (' '+str2+' '):\n",
    "                        num+=1\n",
    "                        total_entries+=(' '+str2+' ').count(' '+word+' ')\n",
    "                        cnt_letters+=(' '+str2+' ').count(' '+word+' ') * (len(word))\n",
    "                        cnt_unique_letters+=(len(word))\n",
    "                        word_list.append(word)\n",
    "                    all_num+=1\n",
    "                    all_total_entries+=1\n",
    "                    all_cnt_letters+=len(word)\n",
    "    \n",
    "    if all_num==0:\n",
    "        ratio_num=0\n",
    "    else:\n",
    "        ratio_num=1.0*num/all_num\n",
    "    \n",
    "    if all_cnt_letters==0:\n",
    "        ratio_letters=0\n",
    "    else:\n",
    "        ratio_letters=1.0*cnt_unique_letters/all_cnt_letters\n",
    "                 \n",
    "    return num, total_entries, cnt_unique_letters, ratio_num, ratio_letters, \" \".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_matcher(s1,s2):\n",
    "    seq=difflib.SequenceMatcher(None, s1,s2)\n",
    "    rt=round(seq.ratio(),7)\n",
    "    l1=len(s1)\n",
    "    l2=len(s2)\n",
    "    if len(s1)==0 or len(s2)==0:\n",
    "        rt=0\n",
    "        rt_scaled=0\n",
    "    else:\n",
    "        rt_scaled=round(rt*max(l1,l2)/min(l1,l2),7)\n",
    "    return rt, rt_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature6(df_):  \n",
    "    df_all = pd.DataFrame()\n",
    "    df_all['word_in_title_tuple']=df_.apply(lambda x: \\\n",
    "                                               str_common_word(x['q1_stemmed'],x['q2_stemmed']),axis=1)\n",
    "    df_all['f_word_in_letratio'] = df_all['word_in_title_tuple'].map(lambda x: x[4])\n",
    "    df_all['seqmatch_tuple']=df_.apply(lambda x: \\\n",
    "                                                seq_matcher(x['q1_stemmed'],x['q2_stemmed']),axis=1)\n",
    "    df_all['f_seqmatch_ratio'] = df_all['seqmatch_tuple'].map(lambda x: x[0])\n",
    "    df_all['f_seqmatch_ratioscaled'] = df_all['seqmatch_tuple'].map(lambda x: x[1])\n",
    "    col = [c for c in df_all.columns if c[:1]=='f']\n",
    "    return df_all[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fea6 = feature6(df_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fea6 = feature6(df_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature7(df_,name,q1,q2):\n",
    "    data = pd.DataFrame()\n",
    "    data['JaccardCoef_2gram'+name] = df_.apply(lambda x: jaccard_coef(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),2)),\n",
    "                                                                     list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),2))),\n",
    "                                                  axis=1)\n",
    "    data['DiceDistance_2gram'+name] = df_.apply(lambda x: dice_dist(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),2)),\n",
    "                                                                     list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),2))),\n",
    "                                                  axis=1)\n",
    "    data['JaccardCoef_3gram'+name] = df_.apply(lambda x: jaccard_coef(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),3)),\n",
    "                                                                     list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),3))),\n",
    "                                                  axis=1)\n",
    "    data['DiceDistance_3gram'+name] = df_.apply(lambda x: dice_dist(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),3)),\n",
    "                                                                     list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),3))),axis=1)\n",
    "#   data['CompressionDistance'] = df_.apply(lambda x: compression_dist(str(x[q1]).decode('utf-8'),str(x[q2]).decode('utf-8')),axis=1)\n",
    "    data['edit_dist'+name] = df_.apply(lambda x: edit_dist(str(x[q1]),str(x[q2])),axis=1)\n",
    "    data['cosine_dist'+name] = df_.apply(lambda x: cosine_d(str(x[q1]).split(),str(x[q2]).split()),axis=1)\n",
    "    data['IntersectCount_2gram'+name] = df_.apply(lambda x: IntersectCount_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),2)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),2))),\n",
    "                                                   axis=1)\n",
    "    data['IntersectRatio_2gram'+name] = df_.apply(lambda x: IntersectRatio_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),2)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),2))),\n",
    "                                                   axis=1)\n",
    "    data['CooccurrenceCount_2gram'+name] = df_.apply(lambda x: CooccurrenceCount_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),2)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),2))),\n",
    "                                                   axis=1)\n",
    "    data['CooccurrenceRatio_2gram'+name] = df_.apply(lambda x: CooccurrenceRatio_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),2)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),2))),\n",
    "                                                   axis=1)\n",
    "    data['IntersectCount_3gram'+name] = df_.apply(lambda x: IntersectCount_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),3)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),3))),\n",
    "                                                   axis=1)\n",
    "    data['IntersectRatio_3gram'+name] = df_.apply(lambda x: IntersectRatio_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),3)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),3))),\n",
    "                                                   axis=1)\n",
    "    data['CooccurrenceCount_3gram'+name] = df_.apply(lambda x: CooccurrenceCount_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),3)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),3))),\n",
    "                                                   axis=1)\n",
    "    data['CooccurrenceRatio_3gram'+name] = df_.apply(lambda x: CooccurrenceRatio_Ngram(list(ngrams(nltk.word_tokenize(str(x[q1]).decode('utf-8')),3)),\n",
    "                                                                      list(ngrams(nltk.word_tokenize(str(x[q2]).decode('utf-8')),3))),\n",
    "                                                   axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train_clean = pd.read_csv('./processed_data/train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_clean = pd.read_csv('./processed_data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_fea_train7 =feature7(df_train_clean,'no_stops','q1_no_stops','q2_no_stops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_fea_test7 =feature7(df_test_clean,'no_stops','q1_no_stops','q2_no_stops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodl_train_fea = pd.read_csv('./processed_data/train_no_dl_fea.csv')\n",
    "nodl_test_fea = pd.read_csv('./processed_data/test_no_dl_fea.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in data_fea_train7.columns:\n",
    "    nodl_train_fea[c] = data_fea_train7[c]\n",
    "for c in data_fea_test7.columns:\n",
    "    nodl_test_fea[c] = data_fea_test7[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodl_train_fea.to_csv('./processed_data/train_no_dl_fea.csv',index=False)\n",
    "nodl_test_fea.to_csv('./processed_data/test_no_dl_fea.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodl_test_fea.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "import operator\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import plot, show, subplot, specgram, imshow, savefig\n",
    "\n",
    "RS = 12357\n",
    "ROUNDS = 315\n",
    "\n",
    "print(\"Started\")\n",
    "np.random.seed(RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def add_word_count(x, df, word):\n",
    "    x['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n",
    "    x['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n",
    "    x[word + '_both'] = x['q1_' + word] * x['q2_' + word]\n",
    "def feature8(train,test):\n",
    "\tdf_train = train.copy()\n",
    "\tdf_test  = test.copy()\n",
    "\tprint(\"Original data: X_train: {}, X_test: {}\".format(df_train.shape, df_test.shape))\n",
    "\n",
    "\tprint(\"Features processing, be patient...\")\n",
    "\n",
    "\t# If a word appears only once, we ignore it completely (likely a typo)\n",
    "\t# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "\tdef get_weight(count, eps=10000, min_count=2):\n",
    "\t\treturn 0.0 if count < min_count else 1.0 / (count + eps)\n",
    "\n",
    "\ttrain_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
    "\twords = (\" \".join(train_qs)).lower().split()\n",
    "\tcounts = Counter(words)\n",
    "\tweights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "\tstops = set(stopwords.words(\"english\"))\n",
    "\tdef word_shares(row):\n",
    "\t\tq1_list = str(row['question1']).lower().split()\n",
    "\t\tq1 = set(q1_list)\n",
    "\t\tq1words = q1.difference(stops)\n",
    "\t\tif len(q1words) == 0:\n",
    "\t\t\treturn '0:0:0:0:0:0:0:0'\n",
    "        \n",
    "\t\tq2_list = str(row['question2']).lower().split()\n",
    "\t\tq2 = set(q2_list)\n",
    "\t\tq2words = q2.difference(stops)\n",
    "\t\tif len(q2words) == 0:\n",
    "\t\t\treturn '0:0:0:0:0:0:0:0'\n",
    "\n",
    "\t\twords_hamming = sum(1.0 for i in zip(q1_list, q2_list) if i[0]==i[1])/float(max(len(q1_list), len(q2_list)))\n",
    "\n",
    "\t\tq1stops = q1.intersection(stops)\n",
    "\t\tq2stops = q2.intersection(stops)\n",
    "\n",
    "\t\tq1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
    "\t\tq2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
    "\n",
    "\t\tshared_2gram = q1_2gram.intersection(q2_2gram)\n",
    "\n",
    "\t\tshared_words = q1words.intersection(q2words)\n",
    "\t\tshared_weights = [weights.get(w, 0) for w in shared_words]\n",
    "\t\tq1_weights = [weights.get(w, 0) for w in q1words]\n",
    "\t\tq2_weights = [weights.get(w, 0) for w in q2words]\n",
    "\t\ttotal_weights = q1_weights + q1_weights\n",
    "\t\t\n",
    "\t\tR1 = np.sum(shared_weights) / float(np.sum(total_weights)) #tfidf share\n",
    "\t\tR2 = len(shared_words) / float(len(q1words) + len(q2words) - len(shared_words)) #count share\n",
    "\t\tR31 = len(q1stops) / float(len(q1words)) #stops in q1\n",
    "\t\tR32 = len(q2stops) / float(len(q2words))#stops in q2\n",
    "\t\tRcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights)).astype(float)*np.sqrt(np.dot(q2_weights,q2_weights)))\n",
    "\t\tRcosine = float(np.dot(shared_weights, shared_weights))/Rcosine_denominator\n",
    "\t\tif len(q1_2gram) + len(q2_2gram) == 0:\n",
    "\t\t\tR2gram = 0.0\n",
    "\t\telse:\n",
    "\t\t\tR2gram = float(len(shared_2gram)) / (len(q1_2gram) + len(q2_2gram))\n",
    "\t\treturn '{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, float(len(shared_words)), R31, R32, R2gram, Rcosine, words_hamming)\n",
    "\n",
    "\tdf = pd.concat([df_train, df_test])\n",
    "\tdf['word_shares'] = df.apply(word_shares, axis=1, raw=True)\n",
    "\n",
    "\tx = pd.DataFrame()\n",
    "\n",
    "\tx['h_word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
    "\tx['h_word_match_2root'] = np.sqrt(x['h_word_match'])\n",
    "\tx['h_tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
    "\tx['h_shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
    "\n",
    "\tx['h_stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
    "\tx['h_stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
    "\tx['h_shared_2gram']     = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\n",
    "\tx['h_cosine']           = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\n",
    "\tx['h_words_hamming']    = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\n",
    "\tx['h_diff_stops_r']     = x['h_stops1_ratio'] - x['h_stops2_ratio']\n",
    "\n",
    "\tx['h_len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n",
    "\tx['h_len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n",
    "\tx['h_diff_len'] = x['h_len_q1'] - x['h_len_q2']\n",
    "\t\n",
    "\tx['h_caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "\tx['h_caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "\tx['h_diff_caps'] = x['h_caps_count_q1'] - x['h_caps_count_q2']\n",
    "\n",
    "\tx['h_len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "\tx['h_len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "\tx['h_diff_len_char'] = x['h_len_char_q1'] - x['h_len_char_q2']\n",
    "\n",
    "\tx['h_len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "\tx['h_len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "\tx['h_diff_len_word'] = x['h_len_word_q1'] - x['h_len_word_q2']\n",
    "\n",
    "\tx['h_avg_world_len1'] = x['h_len_char_q1'] / x['h_len_word_q1']\n",
    "\tx['h_avg_world_len2'] = x['h_len_char_q2'] / x['h_len_word_q2']\n",
    "\tx['h_diff_avg_word'] = x['h_avg_world_len1'] - x['h_avg_world_len2']\n",
    "\n",
    "\tx['h_exactly_same'] = (df['question1'] == df['question2']).astype(int)\n",
    "\tx['h_duplicated'] = df.duplicated(['question1','question2']).astype(int)\n",
    "\tadd_word_count(x, df,'how')\n",
    "\tadd_word_count(x, df,'what')\n",
    "\tadd_word_count(x, df,'which')\n",
    "\tadd_word_count(x, df,'who')\n",
    "\tadd_word_count(x, df,'where')\n",
    "\tadd_word_count(x, df,'when')\n",
    "\tadd_word_count(x, df,'why')\n",
    "\n",
    "\tprint(x.columns)\n",
    "\tprint(x.describe())\n",
    "\n",
    "\tfeature_names = list(x.columns.values)\n",
    "# \tcreate_feature_map(feature_names)\n",
    "# \tprint(\"Features: {}\".format(feature_names))\n",
    "\n",
    "\tx_train = x[:df_train.shape[0]]\n",
    "\tx_test  = x[df_train.shape[0]:]\n",
    "\ty_train = df_train['is_duplicate'].values\n",
    "\tdel x, df_train\n",
    "\treturn x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('train_org.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fea8,test_fea8 = feature8(df_train,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_fea8.to_csv('./processed_data/test_no_dl_fea1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fea8.to_csv('./processed_data/train_no_dl_fea1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "seed = 1024\n",
    "np.random.seed(seed)\n",
    "train = pd.read_csv(\"train_org.csv\")\n",
    "test = pd.read_csv(\"test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stem_str(x,stemmer=SnowballStemmer('english')):\n",
    "    x = text.re.sub(\"[^a-zA-Z0-9]\",\" \", x)\n",
    "    #x = \" \".join([stemmer.stem(z) for z in x.split(\" \")])\n",
    "    #x = \" \".join(x.split())\n",
    "    return x\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "print('Generate porter')\n",
    "train['question1_porter'] = train['question1'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "test['question1_porter'] = test['question1'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "\n",
    "train['question2_porter'] = train['question2'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "test['question2_porter'] = test['question2'].astype(str).apply(lambda x:stem_str(x.lower(),porter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('./processed_data/train_porter.csv')\n",
    "test.to_csv('./processed_data/test_porter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv(path+\"train_porter.csv\")\n",
    "# test = pd.read_csv(path+\"test_porter.csv\")\n",
    "test['is_duplicated']=[-1]*test.shape[0]\n",
    "len_train = train.shape[0]\n",
    "data_all = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import distance\n",
    "def _try_divide(x, y, val=0.0):\n",
    "    \"\"\"try to divide two numbers\"\"\"\n",
    "    if y != 0.0:\n",
    "        val = float(x) / y\n",
    "    return val\n",
    "def calc_set_intersection(text_a, text_b):\n",
    "    a = set(text_a.split())\n",
    "    b = set(text_b.split())\n",
    "    return _try_divide(len(a.intersection(b)) *1.0,len(a))\n",
    "def str_jaccard(str1, str2):\n",
    "\n",
    "\n",
    "    str1_list = str1.split(\" \")\n",
    "    str2_list = str2.split(\" \")\n",
    "    res = distance.jaccard(str1_list, str2_list)\n",
    "    return res\n",
    "\n",
    "# shortest alignment\n",
    "def str_levenshtein_1(str1, str2):\n",
    "\n",
    "\n",
    "    str1_list = str1.split(' ')\n",
    "    str2_list = str2.split(' ')\n",
    "    res = distance.nlevenshtein(str1, str2,method=1)\n",
    "    return res\n",
    "\n",
    "# longest alignment\n",
    "def str_levenshtein_2(str1, str2):\n",
    "\n",
    "    str1_list = str1.split(' ')\n",
    "    str2_list = str2.split(' ')\n",
    "    res = distance.nlevenshtein(str1, str2,method=2)\n",
    "    return res\n",
    "\n",
    "def str_sorensen(str1, str2):\n",
    "    str1_list = str1.split(' ')\n",
    "    str2_list = str2.split(' ')\n",
    "    res = distance.sorensen(str1_list, str2_list)\n",
    "    return res\n",
    "def feature9(df_):\n",
    "    data = pd.DataFrame()\n",
    "    print('Generate intersection')\n",
    "    data['w_interaction'] = df_.astype(str).apply(lambda x:calc_set_intersection(x['question1'],x['question2']),axis=1)\n",
    "    print('Generate porter intersection')\n",
    "    data['w_porter_interaction'] = df_.astype(str).apply(lambda x:calc_set_intersection(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "    print('Generate jaccard')\n",
    "    data['w_jaccard'] = df_.astype(str).apply(lambda x:str_jaccard(x['question1'],x['question2']),axis=1)\n",
    "    print('Generate porter jaccard')\n",
    "    data['w_porter_jaccard'] = df_.astype(str).apply(lambda x:str_jaccard(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "    print('Generate levenshtein_1')\n",
    "    data['w_levenshtein_1']= df_.astype(str).apply(lambda x:str_levenshtein_1(x['question1'],x['question2']),axis=1)\n",
    "    print('Generate porter levenshtein_1')\n",
    "    data['w_porter_levenshtein_1'] = df_.astype(str).apply(lambda x:str_levenshtein_1(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "    print('Generate levenshtein_2')\n",
    "    data['w_levenshtein_2'] = df_.astype(str).apply(lambda x:str_levenshtein_2(x['question1'],x['question2']),axis=1)\n",
    "    print('Generate porter levenshtein_2')\n",
    "    data['w_porter_levenshtein_2'] = df_.astype(str).apply(lambda x:str_levenshtein_2(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "    print('Generate sorensen')\n",
    "    data['w_sorensen'] = df_.astype(str).apply(lambda x:str_sorensen(x['question1'],x['question2']),axis=1)\n",
    "    print('Generate porter sorensen')\n",
    "    data['w_porter_sorensen'] = df_.astype(str).apply(lambda x:str_sorensen(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fea9 = feature9(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_fea9 = feature9(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in train_fea9.columns:\n",
    "    train_fea8[c] = train_fea9[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in test_fea9.columns:\n",
    "    test_fea8[c] = test_fea9[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_fea8.to_csv('./processed_data/test_no_dl_fea1.csv',index=False)\n",
    "train_fea8.to_csv('./processed_data/train_no_dl_fea1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_orig = pd.read_csv('train_org.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_orig = pd.read_csv('test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ques = pd.concat([train_orig[['question1', 'question2']], \\\n",
    "        test_orig[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "ques.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_dict = defaultdict(set)\n",
    "for i in range(ques.shape[0]):\n",
    "        q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "        q_dict[ques.question2[i]].add(ques.question1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_orig['q1_q2_intersect'] = train_orig.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "test_orig['q1_q2_intersect'] = test_orig.apply(q1_q2_intersect, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = train_orig.q1_q2_intersect.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feat = train_orig[['q1_q2_intersect']]\n",
    "test_feat = test_orig[['q1_q2_intersect']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_feat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fea8 = pd.read_csv('./processed_data/test_no_dl_fea1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fea8 = pd.read_csv('./processed_data/test_no_dl_fea1.csv')\n",
    "train_fea8 = pd.read_csv('./processed_data/train_no_dl_fea1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fea8['magicfea2'] = train_feat.q1_q2_intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_fea8['magicfea2'] = test_feat.q1_q2_intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def word_match_share(row, stops=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (float(len(shared_words_in_q1)) + float(len(shared_words_in_q2)))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def jaccard(row):\n",
    "    wic = set(row['question1']).intersection(set(row['question2']))\n",
    "    uw = set(row['question1']).union(row['question2'])\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (float(len(wic)) / len(uw))\n",
    "\n",
    "def common_words(row):\n",
    "    return len(set(row['question1']).intersection(set(row['question2'])))\n",
    "\n",
    "def total_unique_words(row):\n",
    "    return len(set(row['question1']).union(row['question2']))\n",
    "\n",
    "def total_unq_words_stop(row, stops):\n",
    "    return len([x for x in set(row['question1']).union(row['question2']) if x not in stops])\n",
    "\n",
    "def wc_diff(row):\n",
    "    return abs(len(row['question1']) - len(row['question2']))\n",
    "\n",
    "def wc_ratio(row):\n",
    "    l1 = len(row['question1'])*1.0 \n",
    "    l2 = len(row['question2'])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique(row):\n",
    "    return abs(len(set(row['question1'])) - len(set(row['question2'])))\n",
    "\n",
    "def wc_ratio_unique(row):\n",
    "    l1 = len(set(row['question1'])) * 1.0\n",
    "    l2 = len(set(row['question2']))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique_stop(row, stops=None):\n",
    "    return abs(len([x for x in set(row['question1']) if x not in stops]) - len([x for x in set(row['question2']) if x not in stops]))\n",
    "\n",
    "def wc_ratio_unique_stop(row, stops=None):\n",
    "    l1 = len([x for x in set(row['question1']) if x not in stops])*1.0 \n",
    "    l2 = len([x for x in set(row['question2']) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def same_start_word(row):\n",
    "    if not row['question1'] or not row['question2']:\n",
    "        return np.nan\n",
    "    return int(row['question1'][0] == row['question2'][0])\n",
    "\n",
    "def char_diff(row):\n",
    "    return abs(len(''.join(row['question1'])) - len(''.join(row['question2'])))\n",
    "\n",
    "def char_ratio(row):\n",
    "    l1 = len(''.join(row['question1']))*1.0  \n",
    "    l2 = len(''.join(row['question2']))\n",
    "    if l2 == 0.0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def char_diff_unique_stop(row, stops=None):\n",
    "    return abs(len(''.join([x for x in set(row['question1']) if x not in stops])) - len(''.join([x for x in set(row['question2']) if x not in stops])))\n",
    "\n",
    "\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0 / (count + eps)\n",
    "    \n",
    "def tfidf_word_match_share_stops(row, stops=None, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights).astype(float) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def tfidf_word_match_share(row, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights).astype(float) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "\n",
    "def build_features(data, stops, weights):\n",
    "    X = pd.DataFrame()\n",
    "    f = functools.partial(word_match_share, stops=stops)\n",
    "    X['k_word_match'] = data.apply(f, axis=1, raw=True) #1\n",
    "\n",
    "    f = functools.partial(tfidf_word_match_share, weights=weights)\n",
    "    X['k_tfidf_wm'] = data.apply(f, axis=1, raw=True) #2\n",
    "\n",
    "    f = functools.partial(tfidf_word_match_share_stops, stops=stops, weights=weights)\n",
    "    X['k_tfidf_wm_stops'] = data.apply(f, axis=1, raw=True) #3\n",
    "\n",
    "    X['k_jaccard'] = data.apply(jaccard, axis=1, raw=True) #4\n",
    "    X['k_wc_diff'] = data.apply(wc_diff, axis=1, raw=True) #5\n",
    "    X['k_wc_ratio'] = data.apply(wc_ratio, axis=1, raw=True) #6\n",
    "    X['k_wc_diff_unique'] = data.apply(wc_diff_unique, axis=1, raw=True) #7\n",
    "    X['k_wc_ratio_unique'] = data.apply(wc_ratio_unique, axis=1, raw=True) #8\n",
    "\n",
    "    f = functools.partial(wc_diff_unique_stop, stops=stops)    \n",
    "    X['k_wc_diff_unq_stop'] = data.apply(f, axis=1, raw=True) #9\n",
    "    f = functools.partial(wc_ratio_unique_stop, stops=stops)    \n",
    "    X['k_wc_ratio_unique_stop'] = data.apply(f, axis=1, raw=True) #10\n",
    "\n",
    "    X['k_same_start'] = data.apply(same_start_word, axis=1, raw=True) #11\n",
    "    X['k_char_diff'] = data.apply(char_diff, axis=1, raw=True) #12\n",
    "\n",
    "    f = functools.partial(char_diff_unique_stop, stops=stops) \n",
    "    X['k_char_diff_unq_stop'] = data.apply(f, axis=1, raw=True) #13\n",
    "\n",
    "#     X['common_words'] = data.apply(common_words, axis=1, raw=True)  #14\n",
    "    X['k_total_unique_words'] = data.apply(total_unique_words, axis=1, raw=True)  #15\n",
    "\n",
    "    f = functools.partial(total_unq_words_stop, stops=stops)\n",
    "    X['k_total_unq_words_stop'] = data.apply(f, axis=1, raw=True)  #16\n",
    "    \n",
    "    X['k_char_ratio'] = data.apply(char_ratio, axis=1, raw=True) #17    \n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def feature9():\n",
    "    df_train = pd.read_csv('train_org.csv')\n",
    "    df_train = df_train.fillna(' ')\n",
    "    df_test = pd.read_csv('test_final.csv')\n",
    "    ques = pd.concat([df_train[['question1', 'question2']], \\\n",
    "        df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "    q_dict = defaultdict(set)\n",
    "    for i in range(ques.shape[0]):\n",
    "        q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "        q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "\n",
    "    def q1_freq(row):\n",
    "        return(len(q_dict[row['question1']]))\n",
    "        \n",
    "    def q2_freq(row):\n",
    "        return(len(q_dict[row['question2']]))\n",
    "        \n",
    "    def q1_q2_intersect(row):\n",
    "        return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "    df_train['k_q1_q2_intersect'] = df_train.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "    df_train['k_q1_freq'] = df_train.apply(q1_freq, axis=1, raw=True)\n",
    "    df_train['k_q2_freq'] = df_train.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "    df_test['k_q1_q2_intersect'] = df_test.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "    df_test['k_q1_freq'] = df_test.apply(q1_freq, axis=1, raw=True)\n",
    "    df_test['k_q2_freq'] = df_test.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "    test_leaky = df_test.loc[:, ['k_q1_q2_intersect','k_q1_freq','k_q2_freq']]\n",
    "    del df_test\n",
    "\n",
    "    train_leaky = df_train.loc[:, ['k_q1_q2_intersect','k_q1_freq','k_q2_freq']]\n",
    "\n",
    "    # explore\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "    df_train['question1'] = df_train['question1'].map(lambda x: str(x).lower().split())\n",
    "    df_train['question2'] = df_train['question2'].map(lambda x: str(x).lower().split())\n",
    "\n",
    "    train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist())\n",
    "\n",
    "    words = [x for y in train_qs for x in y]\n",
    "    counts = Counter(words)\n",
    "    weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "    print('Building Features')\n",
    "    X_train = build_features(df_train, stops, weights)\n",
    "    X_train = pd.concat((X_train, train_leaky), axis=1)\n",
    "#     y_train = df_train['is_duplicate'].values\n",
    "\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=4242)\n",
    "\n",
    "#     #UPDownSampling\n",
    "#     pos_train = X_train[y_train == 1]\n",
    "#     neg_train = X_train[y_train == 0]\n",
    "#     X_train = pd.concat((neg_train, pos_train.iloc[:int(0.8*len(pos_train))], neg_train))\n",
    "#     y_train = np.array([0] * neg_train.shape[0] + [1] * pos_train.iloc[:int(0.8*len(pos_train))].shape[0] + [0] * neg_train.shape[0])\n",
    "#     print(np.mean(y_train))\n",
    "#     del pos_train, neg_train\n",
    "\n",
    "#     pos_valid = X_valid[y_valid == 1]\n",
    "#     neg_valid = X_valid[y_valid == 0]\n",
    "#     X_valid = pd.concat((neg_valid, pos_valid.iloc[:int(0.8 * len(pos_valid))], neg_valid))\n",
    "#     y_valid = np.array([0] * neg_valid.shape[0] + [1] * pos_valid.iloc[:int(0.8 * len(pos_valid))].shape[0] + [0] * neg_valid.shape[0])\n",
    "#     print(np.mean(y_valid))\n",
    "#     del pos_valid, neg_valid\n",
    "\n",
    "\n",
    "#     params = {}\n",
    "#     params['objective'] = 'binary:logistic'\n",
    "#     params['eval_metric'] = 'logloss'\n",
    "#     params['eta'] = 0.02\n",
    "#     params['max_depth'] = 7\n",
    "#     params['subsample'] = 0.6\n",
    "#     params['base_score'] = 0.2\n",
    "#     # params['scale_pos_weight'] = 0.2\n",
    "\n",
    "#     d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "#     d_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "#     watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "#     bst = xgb.train(params, d_train, 2500, watchlist, early_stopping_rounds=50, verbose_eval=50)\n",
    "#     print(log_loss(y_valid, bst.predict(d_valid)))\n",
    "#     bst.save_model(args.save + '.mdl')\n",
    "\n",
    "\n",
    "#     print('Building Test Features')\n",
    "#     df_test = pd.read_csv('../data/test_features.csv', encoding=\"ISO-8859-1\")\n",
    "#     x_test_ab = df_test.iloc[:, 2:-1]\n",
    "#     x_test_ab = x_test_ab.drop('euclidean_distance', axis=1)\n",
    "#     x_test_ab = x_test_ab.drop('jaccard_distance', axis=1)\n",
    "    \n",
    "    df_test = pd.read_csv('test_final.csv')\n",
    "    df_test = df_test.fillna(' ')\n",
    "\n",
    "    df_test['question1'] = df_test['question1'].map(lambda x: str(x).lower().split())\n",
    "    df_test['question2'] = df_test['question2'].map(lambda x: str(x).lower().split())\n",
    "    \n",
    "    x_test = build_features(df_test, stops, weights)\n",
    "    x_test = pd.concat((x_test, test_leaky), axis=1)\n",
    "#     d_test = xgb.DMatrix(x_test)\n",
    "#     p_test = bst.predict(d_test)\n",
    "#     sub = pd.DataFrame()\n",
    "#     sub['test_id'] = df_test['test_id']\n",
    "#     sub['is_duplicate'] = p_test\n",
    "#     sub.to_csv('../predictions/' + args.save + '.csv')\n",
    "    return X_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_9,test_9= feature9()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fea9 = pd.read_csv('./processed_data/test_no_dl_fea1.csv')\n",
    "train_fea9 = pd.read_csv('./processed_data/train_no_dl_fea1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fea9.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in train_9.columns:\n",
    "    train_fea9[c] = train_9[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_fea9.to_csv('./processed_data/train_no_dl_fea1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in test_9.columns:\n",
    "    test_fea9[c] = test_9[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fea9.to_csv('./processed_data/test_no_dl_fea1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_fea9.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"I do not like green eggs and ham, I do not like them Sam I am!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ngrams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-105295c3b1bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ngrams' is not defined"
     ]
    }
   ],
   "source": [
    "list(ngrams(nltk.word_tokenize(text),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = nltk.wordpunct_tokenize(text)\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "sorted(bigram for bigram, score in scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "text = \"Hi How are you? i am fine and you\"\n",
    "token=nltk.word_tokenize(text)\n",
    "bigrams=ngrams(token,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
